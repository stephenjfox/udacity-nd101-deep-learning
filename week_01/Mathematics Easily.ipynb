{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fields of Mathematics for Deep Learning\n",
    "1. Linear Algebra\n",
    "2. Statistics\n",
    "3. Calculus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Four Steps of a Deep Learning Pipeline\n",
    "1. Collect Data\n",
    "2. Build Model\n",
    "3. Train Model\n",
    "4. Test Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Compare this with...*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Systematic Process For Working Through Predictive Modeling Problems\n",
    "*That delivers above average results* ([Source](http://machinelearningmastery.com/process-for-working-through-machine-learning-problems/))\n",
    "1. Define the Problem\n",
    "2. Prepare the Data\n",
    "3. Spot Check Algorithms\n",
    "4. Improve Results\n",
    "5. Present Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's much here that be mapped to the previous process.\n",
    "- I leave you to realize it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "1. Clean Data\n",
    "  - Drop empty values\n",
    "  - Remove unnecessary features\n",
    "2. Normalizing data\n",
    "  - Helps the model reach convergence by causing all neurons in the network to operate on the same scale\n",
    "  - Working in the same scale (or 'range') definitively gives nodes/neurons less to choose from, increasing probabilities for more appropriate values by default\n",
    "3. Adapt Data\n",
    "  - Insure the data is in a format the network will accept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Vocabulary:\n",
    "  - **Convergence (point)**. The point where error of the prediction function approaches its asymptotic minimum.\n",
    "  - **Feature**. A dimension of some observed occurence (aka 'data').\n",
    "    - Examples: brain weight of an animal, height of a human, horse power of a car\n",
    "  - **Adapt**. To translate, massage, trim, or otherwise re-package your data for consumption.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization via 'min-max' scaling\n",
    "Given:\n",
    "- Some domain, X\n",
    "- Some element of X, x\n",
    "- A pair of functions, `max` and `min`\n",
    "\n",
    "The equation will normalize an `x` relative to its neighbors\n",
    "- This is vaguely familiar.\n",
    "  - The equations for correlation of data (to the line of best fit) use something like this process.\n",
    "  - Because each data point has a distance to the LoBF, you need a similar scaling to find that \"distance\"\n",
    "    - The closer a point to the LoBF, the higher the correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "z = \\frac{x - min(X)}{max(X) - min(X)}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adapting Data\n",
    "\n",
    "Quick jump to Linear Algebra\n",
    "- Some terms that are brought up a **lot**\n",
    "  1. Scalars - single number\n",
    "  2. Vector - multiple numbers in one dimension\n",
    "    - Funnily enough, vectors signify dimensionality in multi-variable (Newtonian) Calculus and any Physics class.\n",
    "  3. Matrix - Multiple numbers in two dimensions\n",
    "  4. Tensor - Numbers in *n* dimensions\n",
    "\n",
    "Simple ideas can have complex or simple explanations\n",
    "- For example: 1 is a \"number\". Clearly.\n",
    "  - But we also call it an \"integer\" and a \"real\" number (not on the imaginary plane).\n",
    "  - It can even be a \"floating point\" number, with no value after the decimal point to represent.\n",
    "- From this, we can conclude that more *rudamentary* ideas can be encapsulated in more *exceptional* forms of categorization.\n",
    "\n",
    "**This is why** we use *Tensors*: because the other three categories can **all** be represented as a Tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "***In other words***\n",
    "Tensors: N-Dimensional Matrices\n",
    "- 0-Dimensional --> Scalar\n",
    "- 1-Dimensional --> Vector\n",
    "- 2-Dimensional --> Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "We'll now start to place with a neural network.\n",
    "\n",
    "...\n",
    "Some list that Siraj is putting out (in a really crappy song)...\n",
    "\n",
    "Looks to be \"How a Neural Network works\", or something to that effect.\n",
    "\n",
    "1. Normalize\n",
    "2. Learning Hyperparameters\n",
    "3. Initializing Weights\n",
    "4. Forward Propogation\n",
    "5. Calculate Error\n",
    "6. Back propogate\n",
    "  - Rebalance weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Collect Data\n",
    "x = np.array([\n",
    "    [0,0,1],\n",
    "    [0,1,1],\n",
    "    [1,0,1],\n",
    "    [1,1,1]\n",
    "])\n",
    "y = np.array([\n",
    "    [0],\n",
    "    [1],\n",
    "    [1],\n",
    "    [0]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1]\n",
      " [0 1 1]\n",
      " [1 0 1]\n",
      " [1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Vocabulary:\n",
    "  - **Hyperparameters**. High-level tunings of the network itself. Changes the resultant neural network in such ways as:\n",
    "    - Learning rate\n",
    "    - How many hidden layers\n",
    "    - ***Please note***: these will be increasingly important as the network grows in size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The next two minutes (from 4:00 to ~6:30) are unfairly fast. He's skipping the math and just putting numbers up, which is not helpful.\n",
    "- On the plus side, he gave a great intuitive explanation of *Gradient Descent*...\n",
    "\n",
    "**Gradient Descent**\n",
    "Roughly: an iterative process of approximation (very much like Calculus derivatives), for the purpose of reaching a 0-slope (\"no slope\") in our graph, thus minimizing error.\n",
    "- I say \"thus\" because a positive slope is a positive error and negative slope a negative error\n",
    "  - Only at no-slope is there no error.\n",
    "- **We use this** to find a sense of direction in calculating error of predictions (as explained above) in **modifying our weights**\n",
    "  - Thus shorting the iterations the network must take to complete the job\n",
    "\n",
    "... Also, the **gradient** is a the **slope at a *point***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "It's been an hour (it is now 12:52 AM) since I started this notebook.\n",
    "\n",
    "Thankfully, Siraj made a [\"How to Make a Neural Network\"](https://www.youtube.com/watch?v=p69khggr1Jo&index=3&list=PL2-dafEMk2A7YdKv4XfKpfbTH5z6rEEj3)\n",
    "\n",
    "Further proof that this course isn't perfect: \"Intro to Neural Networks\" is two lessons away. If it's an hour-per-lesson, I'll be caught up in no time.\n",
    "\n",
    "If not... Well, I'm gonna rock it and get this first project done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Thankfully\n",
    "\n",
    "[Deep Learning Book](http://www.deeplearningbook.org/contents/intro.html) exists!\n",
    "- The intro maps out how to read the book\n",
    "  - It even says that it's meant to get developers up to speed, so they can have a machine learning and AI centric career\n",
    "  - This is a segment of my exact goal.\n",
    "\n",
    "And if I get to a good place, there is [a challenge to build a neural network](https://github.com/llSourcell/how_to_do_math_for_deep_learning) that I hope will bolster my capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, for becoming a Data Scientist, I'm hopeful that the following posts will prove insightful and not condescendent or redundant\n",
    "- [ML for Programmers](http://machinelearningmastery.com/machine-learning-for-programmers/)\n",
    "- [ML Mastery Method](http://machinelearningmastery.com/machine-learning-mastery-method/)\n",
    "\n",
    "---\n",
    "Then again, I probably don't need them at all. I've got a rhythm going and I don't need to keep looking at myself like an __absolute__ beginner.\n",
    "- Much like I treat myself with foreign languages.\n",
    "- The guard-rail is nice, but I've out grown the training wheels."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
