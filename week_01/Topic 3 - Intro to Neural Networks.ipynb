{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using an example very similar to that of the \"Intro to Machine Learning\" lesson that covers *data surfaces*, we learn this process of determining whether a point *passes* or *fails* a tast (based on historics) is __Logistic Regression__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The example (with the similar data surface) is of student grades and entrance test scores, colored for acceptance\n",
    "  - green dots on the right hand of the D.S. are (supposedly) accepted\n",
    "  - red dots on the left hand are subsequently rejected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to think about this is: \"how easily can we separate the data?\" (with a line)\n",
    "- In this lesson, we'll gradually reduce our losses via __gradient decent__\n",
    "  - The *log-loss function* will have an output whose error we want to minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer this question:\n",
    "  - In which direction can we rotate (or move) the line for the maximum error reduction?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> It looks like we're proposing too many acceptances, because some of these students have low grades with good test scores...\n",
    "\n",
    "> Hmmmm.\n",
    "\n",
    "> Maybe a cirle?\n",
    "\n",
    "> How about two lines? Yes, __let's use two lines__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But how do we find the two lines?\n",
    "- Gradient descent still does the job.\n",
    "- We'll calculate against the log-loss function, looking for two different positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is a neural network\n",
    "1. Is this point over the horizontal line?\n",
    "2. Is this point over the vertical line?\n",
    "3. Are the answers to both 1 and 2 yes?\n",
    "  - This yields only a single \"yes\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This feels like the makings of a perceptron...\n",
    "- Doing bare-bones, basic boolean ANDs\n",
    "\n",
    "I'm wondering how we translate this truth-table-like problem into a system of nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then teach breaks it down for us ;)\n",
    "> Let's graph each question as a small node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A breakdown\n",
    "(To better form an intuition?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Is the point over the horizontal line?\n",
    "    - And two input nodes: test score and grades\n",
    "    - We can plot this like coordinate pair\n",
    "    - Outputs a Boolean\n",
    "2. Is the point over the vertical line?\n",
    "    - Ouputs a Boolean to answer the question\n",
    "3. Add a node (the __output node__), one layer \"in\"?\n",
    "    - This node receives the previous two inputs and logical ANDs them together\n",
    "    - Ouputs the result of that boolean addition(?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "But __how__ is this a neural network?\n",
    "\n",
    "Well just look at the layers like this:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{vmatrix}\n",
    "Test: & 1 \\\\\n",
    "Grade: & 8\n",
    "\\end{vmatrix} {_{->}^{->}}\n",
    "\\begin{vmatrix}\n",
    "horizLineTestNode \\\\\n",
    "vertLineTestNode\n",
    "\\end{vmatrix} {_{->}^{->}}\n",
    "\\begin{vmatrix}\n",
    "AND\n",
    "\\end{vmatrix} ->\n",
    "Output\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- picture the *test* and *grade* flowing into both test nodes (four arrows instead of two)\n",
    "- That's the general idea\n",
    "  - Data flows left-to-right\n",
    "  - Remember: a given neuron takes, as input, the output of other neurons\n",
    "    - We use numbers at this level\n",
    "    - But soon, we'll do so much more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I really don't want to type out a truth table, or a binary grid\n",
    "  - Please just remember this\n",
    "  - Or just review [this](http://kias.dyndns.org/comath/21.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Vocabulary:\n",
    "  - __Perceptrons__, or neurons, are individual nodes in an interconnected network\n",
    "    - They are the basic unit of neural networks\n",
    "    - Function: *Look at input data and decide how to categorize said data*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the previous example of school acceptance\n",
    "- inputs either passed the threshold for grades and test scores, or didn't\n",
    "- The outputs - \"yes\" and \"no\" - were the __categories__\n",
    "\n",
    "Those categories then combine to produce a decision!\n",
    "- Back example land, whether or not the student is granted admission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "But wait a second....\n",
    "\n",
    "How the heck do these nodes know whats important in checking that threshold?\n",
    "> When initialized, we don't know what information will be most important.\n",
    "\n",
    "> So... We have the network __learn for itself__. And even let it adjust how it considers the data.\n",
    "\n",
    "All of this is done with a little something called..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "My turn.\n",
    "\n",
    "Perceptrons (aka \"neurons\", \"nodes\") have weights for all the inputs that they receive\n",
    "- Think of this like a person who values certain peoples' opinions more than others\n",
    "\n",
    "Based on the inputs (and associated weights), an output can be determined.\n",
    "- And since neurons will only be connected to a set number of other neurons, it is safe to have a fixed number of weights!\n",
    "  - It all makes sense in my brain: \n",
    "    - each \"opinion\" has more/less sway on the node, in its categorization\n",
    "  - Let's see how the course explains it.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> When input data is received, it get multiplied by a weight that is assigned to this particular input.\n",
    "- If we have our example of school acceptance\n",
    "  - `tests` could be an input name for test scores\n",
    "  - `grades` could be an input name for the student's average grades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Pretty much what I said, with a different tone of voice, and vocabulary.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note__ that neuron's given weight for some input starts out random (adding up to 1.00? because percentages???)\n",
    "- Overtime, as the neural network learns more about the kind of input data that leads to student acceptance\n",
    "  - The network adjusts weights based on erors in categorization (from previous results)\n",
    "  - This is called __training__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabulary:\n",
    "  - __training__: The process of improving a network's individual node-level weights, to produce the desired output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- An __extreme__ example of weights would be if the test scores had *no affect at all* on the university acceptance\n",
    "  - The weight of \"test score\"-input would be zero and have no affect on the output of the perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Input (at the perceptron level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to review:\n",
    "\n",
    "- Each input has a weight that represents its importance\n",
    "- Weights are determined during the learning process (aka, __training__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summing the Input Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note that weights will *always* be represented by some type of the letter __w__. It will be capitalized - __W__ - when it represents a __matrix__\n",
    "- Subscript will specify *which* weights.\n",
    "\n",
    "Just remember the variable naming conventions: both math and code\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "They set this equation as the relationship between weights and inputs:\n",
    "\n",
    "\\begin{equation}\n",
    "w_{grades} \\cdot x_{grades} + w_{test} \\cdot x_{test} = -1 \\cdot x_{grades} - 0.2 \\cdot x_{test}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The perceptron applies these weights to the inputs and sums them in a __linear combination__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The next section is verbose as it tries to skirt around the math notation - it's just summations\n",
    "- Of `x_input` and `w_input`, which is the __input__ and the __weight__ associated with said __input__ \n",
    "  - `x` differentiates it, nothing more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Summatively, that looks like:\n",
    "\n",
    "\\begin{equation}\n",
    "\\sum_{i=1}^m w_{i} \\cdot x_{i}\n",
    "\\end{equation}\n",
    "\n",
    "- Where *m* is the number of inputs\n",
    "  - This can be omitted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Please Note\n",
    "that the relative size of weights is what's most important, not the weights themselves\n",
    "\n",
    "- He uses an example of\n",
    "  - w_grades = -1\n",
    "  - w_test = -.2\n",
    "- And points out that the ratio of 5:1 is what's important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__In brief__, multiply an input by its weight\n",
    "\n",
    "- Take an opinion by its importance factor\n",
    "- It's that easy.\n",
    "  - Now to learn about these activation functions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Output: an Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First the analogy that makes the most since in my mind:\n",
    "> Consider an actual neuron in the brain:\n",
    "- It absorbs charge from its neighbors\n",
    "- After a certain amount of input, it discharges\n",
    "\n",
    "> When the activation function fires, the neuron discharges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__He shows__ an activation function as mearly a conditional function:\n",
    "\n",
    "\\begin{equation}\n",
    "f(h) = \\{_{1 if h \\geq 0}^{0 if h \\lt 0}\n",
    "\\end{equation}\n",
    "\n",
    "- Apparently, this function is called the __Heaviside Step Function__\n",
    "  - This sucks for university acceptance :P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "This guy is really making this seem too easy.\n",
    "- I get that the math is simple, but his round-about way of explaining it is frustrating\n",
    "  - I know how to graph on a chart!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we've written a dodgy function (only poor students activate). We'll fix it\n",
    "- The instructor's finally getting to the point!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias, or \"shifting the goods\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have a good shape (to the graph of acceptable inputs), but we don't like the selection.\n",
    "- Just add __bias__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Vocabulary:\n",
    "\n",
    "  - __bias__ is a translation of the graph of acceptable inputs.\n",
    "    - Simple *adding* to the result of the base equation\n",
    "    - First demonstrated on the y-axis.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "For feedback:\n",
    "\n",
    "> This feels like a bad way to explain this. Sure, not everyone remembers mathematics as I do. But this doesn't do the equations justice, and I think it would be inappropriate for students to feel confident with these hand-wave-y definitions\n",
    "\n",
    "- The math is clear, but the words are __not__.\n",
    "  - Almost has me doubting my math\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More to my point, I'm just going to copy the instructors paragraph..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Note to self:\n",
    "\n",
    "- I keep slipping in and out of the Feyman techinque.\n",
    "- I want to rearticulate it, but there's nothing to say\n",
    "  - __ADD__ a bias. Literally.\n",
    "    - Some number, by the science of neural networks, will be generated to shift the good shape to have the right coverage\n",
    "  - I'm not going to re-explain [what summation is](https://en.wikipedia.org/wiki/Summation)\n",
    "    - I've been familiar with this concept for longer than I've been programming.\n",
    "    - Here's my attempt at it:\n",
    "      - For every *single* number between \\*points to start and end of the summation\\*\n",
    "      - Solve the equation with __that__ number as a result\n",
    "      - Add __all__ of those results together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![perceptron formula](https://d17h27t6h515a5.cloudfront.net/topher/2017/February/58951180_perceptron-equation-2/perceptron-equation-2.gif \"Perceptron Formula\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the aformentioned, poor, presumptive summary:\n",
    "\n",
    "> This formula returns $1$ if the input ($x_{1}$,$x_{2}$,...,$x_{m}$) belongs to the accepted-to-university category or returns 0 if it doesn't. The input is made up of one or more real numbers, each one represented by $x_i$, where $m$ is the number of inputs.\n",
    "\n",
    "Then the neural network starts to learn! Initially, the weights ( $w_{i}$ ) and bias ( $b$ ) are assigned a random value, and then they are updated using a learning algorithm like gradient descent. The weights and biases change so that the next training example is more accurately categorized, and patterns in data are \"learned\" by the neural network.\n",
    "\n",
    "Now that you have a good understanding of perceptions, let's put that knowledge to use. In the next section, you'll create the AND perceptron from the Neural Networks video by setting the values for weights and bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While I feel like I'm growing comfortable with Neural Networks, I'm seriously disappointed in this section\n",
    "- \"Uhhh... We can't figure out how to teach this to you, so we're going to give you an impartial __text__ lecture of that is heavy on mathematics, but without the explanations of their necessity\"\n",
    "- It's like Siraj's videos, but less goofy; more stoic\n",
    "  - That is __not__ a good thing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quizzes, or \"suffering will be your teacher\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "This first quiz feels almost unfair, given the abyssmal wrap up of the last section.\n",
    "\n",
    "- I'm about to read another resource to get together a more decent understanding of the activation.\n",
    "  - I can appreciate the abiguity of \"you define it however you want\", but what it this threshold for effective\n",
    "    - How could you ever know before hand?\n",
    "  - I don't imagine the \"Heaviside step function\" will always suffice for an activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a \"fill in the blank\", with the weights of the inputs, a bunch of graphs showing desired behavior, and frustration...\n",
    "- They want the AND function first..\n",
    "  - It's contrived instruction, to get us used to using the $b$\n",
    "- `AND` can be programmed much more simply."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "So, some how, changing the weights and bias for these logic gate neural networks is relaxing my mind.\n",
    "\n",
    "- My brain is just treating them like puzzles.\n",
    "  - I still don't know if this is a good thing\n",
    "- After I gave the answer *it wanted* for \"how would we shift this graph to go from AND to OR?\"\n",
    "  - For the record: \"Increase the weights\" and \"Decrease the magnitude of the bias\"\n",
    "\n",
    "__ALL__ of these are still using the aforementioned biased step function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "Back at it:\n",
    "> A neural network is like any tool. You have to know when to use it.\n",
    "\n",
    "Thankful they didn't make me play the \"weight puzzle game\" with XOR - it's always a pain.\n",
    "- Thankfully, the XOR is a composition of\n",
    "  - two NOT gates\n",
    "  - two AND gates\n",
    "  - one OR gate\n",
    "  - a few passthroughs to give the illusion of \"layers\".\n",
    "- It's __very__ similar to the circuit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The power of a neural network isn't building it by hand, like we were doing. It's the ability to learn from examples. In the next few sections, you'll learn how a neural networks sets it's own weights and biases.\n",
    "\n",
    "... Yeah. Hopefully.\n",
    "- I feel like I've learned a lot of vocabulary, and a few theoretical applications of math I've known for ages...\n",
    "  - And refreshed my Linear Algebra, which is always good.\n",
    "- I've seen glimpses of the applications of that Matrix Math in these neural systems\n",
    "  - I want to weild that power...\n",
    "  - There, I said it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mat returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "He gives a breakdown of all that goes into a neuron:\n",
    "- Inputs\n",
    "  - And associated weights\n",
    "- A bias\n",
    "- resultant input, $h$\n",
    "- Activation function $f(h)$\n",
    "- Output, $y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drawn as such:\n",
    "\n",
    "![image](https://d17h27t6h515a5.cloudfront.net/topher/2017/February/589366f0_simple-neuron/simple-neuron.png \"Cirles are units, boxes are operations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is rather decent, leading to some insights:\n",
    "\n",
    "- Construction of the activation function argument\n",
    "  - Weighted input, with bias\n",
    "  - In other words: $x_1 \\cdot w_1 + b$\n",
    "- The argument to the activation function: $h$\n",
    "- The exit - and output - is __activation function__: $y = f(h)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "My mind understands composition.\n",
    "\n",
    "This a layer, a composition, of trivial operations\n",
    "\n",
    "- Multiplication\n",
    "- Addition\n",
    "- Identity\n",
    "\n",
    "... Along with some arbitrary activation function.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mat has us program the [Sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function) and they keep repeating this:\n",
    "\n",
    "> stacking units will let you model linearly inseparable data, impossible to do with regression models.\n",
    "\n",
    "This phrase \"linearly inseperable data\" has been said four times now.\n",
    "- I haven't heard it before\n",
    "- But I don't think many operations a above decomposition.\n",
    "  - I actually think there are infinitely many operations which can be decomposed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "I'm not going to run the code, but here it is:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    # TODO: Implement sigmoid function\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "inputs = np.array([0.7, -0.3])\n",
    "weights = np.array([0.1, 0.8])\n",
    "bias = -0.1\n",
    "\n",
    "# TODO: Calculate the output\n",
    "output = sigmoid(np.dot(weights, inputs) + bias)\n",
    "\n",
    "print('Output:', output)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "## Learning Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we don't know the correct weights, we have to learn them from history.\n",
    "\n",
    "- Example data\n",
    "\n",
    "Then we can then predict (produce outputs) with those weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Know when you're wrong\n",
    "\n",
    "Since we start with random weights, we need to adjust those weights towards what is proper\n",
    "\n",
    "- *iterate*, if you will"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__So we *measure*__ how wrong we are, to gather some sense of how to course correct\n",
    "\n",
    "- i.e. \"Too big\", \"too big\", \"too hot\", \"too cold\", \"just right\"\n",
    "  - Thanks little Red\n",
    "\n",
    "One of the best methods for doing this is the \"[Sum of Squared Errors](https://en.wikipedia.org/wiki/Residual_sum_of_squares)\":\n",
    "\n",
    "\\begin{equation}\n",
    "E = \\frac 1 2 \\sum{_\\mu} \\sum{_j} [y_j^\\mu - \\hat{y}_j^\\mu]^2\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's break it down\n",
    "\n",
    "Pseudo-academia (me):\n",
    "> Take half of the sum of the sum of:\n",
    "  - all outputs (predictions) subtracted from the true value for some given data point $\\mu$ and a predction $j$ squared\n",
    "\n",
    "Layman's terms:\n",
    "> For every data point (in our training set) calculate the inner sum of \n",
    "- The squared difference of\n",
    "- The true value (categorization [think back]) of a data point, and the prediction for that data point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "We're building a graph and we need to get the shape right.\n",
    "- The squaring is probably inherited from statistical models that fit to a curve. \n",
    "  - Or correlation against a best fit line\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "오후 11시 9분 끝났습니다...\n",
    "\n",
    "Having studied hard, had an awful work day, and endured ill-suited-for-me instruction (\"intellectual babble\", that uses all the right words sans substance), I just want to go to sleep.\n",
    "- My chest is heavy.\n",
    "  - It hurts to feel this much angst\n",
    "  - It's disappointing that getting started is such a labor\n",
    "- Was this really worth \\$600?\n",
    "  - When I've got so many things I could have read for free\\?\n",
    "  - When the, arguably, most important lesson in this course isn't singing true like the previous math lesson\\?\n",
    "    - 왜? 어떡해? :("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2017년 4월 20일 - Reviewing\n",
    "-\n",
    "\n",
    "First off: yes, it is worth it.\n",
    "\n",
    "While the teaching may not be my style - \"look what we can do. And here's some math... Ooo! Shiny, cool things\" - there is genuinely good content here.\n",
    "\n",
    "- The lesson on matrices and Linear Algebra is learning a semester's worth of work in a few hours.\n",
    "  - Concise and effective\n",
    "- Mat's visual-driven lessons are actually effective, which I'm grateful for.\n",
    "  - Human brains are naturally potent at visual comprehension\n",
    "    - Which is why teaching a computer to see is so dang impressive\n",
    "\n",
    "What I need is patience with this, and do what I do best:\n",
    "\n",
    "- Put it together in my head, in a way that makes sense to me.\n",
    "  - This curriculum is designed to hit for everyone\n",
    "  - Not like [some courses](http://neuralnetworksanddeeplearning.com/chap1.html#perceptrons), which cater to 내 스타일"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Let's talk about that other \"course\" (it's a book) for a moment.\n",
    "\n",
    "In about 10 minutes of reading, he has addressed almost __everything__ that this Udacity course has waved its hands at\n",
    "\n",
    "- Node: perceptron and Sigmoid neuron are __different__\n",
    "  - Perceptron does binary output\n",
    "  - Sigmoid neuron outputs the result of a Sigmoid against some input\n",
    "- Gradient descent\n",
    "  - He doesn't refer to it like we already know what it is\n",
    "  - Instead, he talks about the math and it's utility.\n",
    "- Others that I (hopefully) will fill out later.\n",
    "\n",
    "At work, I tried to employ the Feynman technique to teach all that I know about Neural Networks.\n",
    "\n",
    "- I will attempt to do so again here...\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks (Feynman self-Review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A neural network is a system of interconnected (specialized) neurons, organized in columns we call __layers__.\n",
    "\n",
    "- The two most common types of neuron (henceforth, \"node\") are:\n",
    "  - Perceptron, which outputs binary (typically via some Heaviside Step function)\n",
    "  - Sigmoid neuron, which outputs the result of a Sigmoid (see above) function\n",
    "\n",
    "__Perceptrons__, thanks to their binary nature, can parallel any logical operation.\n",
    "\n",
    "- They can become an AND, NAND, OR, or XOR gate\n",
    "  - Parallel to circuitry, and equally composable\n",
    "- Because this logical similarity, neural networks can perform any computation\n",
    "  - Treating the nodes as logic gates\n",
    "    - In-memory circuitry, if you will\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Hypothesis: This proved to be really slow, which is why we didn't get very far with neural network-based Machine Learning for decades.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Perceptron) Nodes take input, and __categorize__ it: determine it exceeds a threshold\n",
    "\n",
    "- That \"determining\" process is just a '$\\geq$' comparison; nothing complicated\n",
    "- Nodes treat their input as humans take opinion's\n",
    "  - \"With a grain of salt\"\n",
    "  - A unique weight factor (\"factor\", think \"multiplication\") is applied to each input\n",
    "    - $x_{input1}$, is the actual input\n",
    "      - Mathematicians love this variable. It's so easy to write ;)\n",
    "    - $w_{input1}$, is the weight associated with it\n",
    "    - Let's use an intermediary $h$ to represent that product of $x_{input1} * w_{input1}$\n",
    "  - The result, $h$, and the some (optional) bias $b$ are added together\n",
    "- Now __we finally have__ something number to compare\n",
    "  - $h + b \\ge 0$? output $1$\n",
    "  - otherwise, output $0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A human analogy:\n",
    "\n",
    "- I think Bob is a bit of a tosspot, so I don't really value what he says\n",
    "  - \"I'm only going to pay partial attention when he's talking to me\"\n",
    "- However, Jenny has been my crush for 3 years, so she has more sway\n",
    "  - \"She might say she'll date me, so I'm going to listen hard\"\n",
    "\n",
    "\n",
    "Bob's *input* is undervalued __relative__ to Jenny. That's the extent of inter-input-relation\n",
    "\n",
    "- One input means nothing to another"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "This gets marginally more complicated with more inputs\n",
    "\n",
    "- Most networks have multiple inputs, which are all processed by all nodes\n",
    "- In this case, we take the summatino of all our \"$h + b$\"-s\n",
    "  - We usually write it in the long form as: $\\sum_{i=1}{(x_i * w_i) + b}$\n",
    "    - Where $i$ in the equation $x_i * w_i$ is a give input value in a list of inputs values\n",
    "      - Remember, we've got multiple inputs here\n",
    "      - With programming, this would be like an array index\n",
    "    - We swap $h$ back out for what it's equal to\n",
    "  - This can also be expressed as a dot product\n",
    "    - $weights = [1, 1, 3]$.\n",
    "      - Let's say these are Bob, Joe, and Jenny's opinions\n",
    "    - $inputs = [5, 6, 3]$\n",
    "      - Let's say these are the number of words in their responses\n",
    "        - And a certain three-word sentence would make someone's life\n",
    "    - $bias = 3$, in a \"learning mood\"\n",
    "    - $weights \\cdot inputs = 1*5 + 1*6 + 3*3$ = 20\n",
    "  - Now we just test that $20$ against our threshold, and we've got output!\n",
    "- That's all there is to calculation.\n",
    "  - The funny part is when we get to *calculating weights on our own*\n",
    "  - No foreknowledge.\n",
    "    - Just your wits about you\n",
    "    - And math ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__There are an infinitely many__ number of ways to approximate appropriate weights.\n",
    "\n",
    "BUT BEFORE ANYTHING ELSE, you have to have effective data to learn from. Otherwise, we're just going crazy here.\n",
    "\n",
    "Here is what I know:\n",
    "\n",
    "1. Our guesses are initially random\n",
    "2. They are wrong to *some degree*\n",
    "3. We must correct against the wrongness\n",
    "\n",
    "\n",
    "Let's look at each of these in turn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1. Initially random__\n",
    "\n",
    "- We really just pick a number\n",
    "  - The idea is \"you gotta start somewhere\"\n",
    "- Very simple to implement programmatically\n",
    "```python\n",
    "import random as rand\n",
    "print(rand.randint(0,10))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2. Our guess is wrong *to some degree*__\n",
    "\n",
    "It would be amazing if our guess were always right.\n",
    "\n",
    "- Heck, that's practically what we're trying to build here, with the network\n",
    "\n",
    "But, it is very likely that our guess is __not__ right.\n",
    "\n",
    "To measure how wrong we are several calculations can be employed.\n",
    "\n",
    "One such calculation is the \"Sum of Squared Errors\", which is really a sum of sums, but that's over complicating the matter.\n",
    "\n",
    "1. We take the difference of our guessed output (for a given input) and the \"true\" output\n",
    "  - $y - \\hat{y}$, where $y$ is the true output and $\\hat{y}$ is a guess\n",
    "2. Square it\n",
    "  - Why?\n",
    "    - So we can have __a.__ a positive number to work with and __b.__ accentuate big differences\n",
    "3. Do this for every output node of the network, then sum them\n",
    "  - Yes, there can be multiple output nodes\n",
    "    - Not ___really___ in perceptrons\n",
    "  - This is that first sum\n",
    "4. Do this for all input-weight pairs (which should come from a bunch of data points), and __sum__ them\n",
    "  - This is that second summation\n",
    "\n",
    "And here's the crazy math equation, where $E$ is \"error\":\n",
    "\n",
    "\\begin{equation}\n",
    "E = \\frac 1 2 \\sum_{\\mu} \\sum_j [y_j^{\\mu} - \\hat{y}_j^{\\mu}]^2\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "__Think about how this relates__ to the perceptron equations, and it actually makes sense:\n",
    "\n",
    "A perceptron only has __1__ output node, so we only did that summation ___once___.\n",
    "\n",
    "- If there had been more output nodes, we would have done it those $m$ times!\n",
    "\n",
    "Eureka\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3. We must correct against our wrongness__\n",
    "\n",
    "One of the most common methods of correction is (stocastic) __Gradient Descent__\n",
    "\n",
    "- I haven't learned this yet, but I do know a few things about it\n",
    "\n",
    "Namely:\n",
    "\n",
    "- It effectively test extremes in a \"too hot, too cold\" manner\n",
    "  - It hops side-to-side, downwards towards the bottom of the \"well\" of this curve\n",
    "  - Every time it jumps, it decreases the intensity, so as to not jump *strictly* side-to-side\n",
    "- It is sensitive to local minima:\n",
    "  - ![caveat local minima](https://d17h27t6h515a5.cloudfront.net/topher/2017/January/587c5ebd_local-minima/local-minima.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determining which is \"correct\" is difficult from this image, but there are __two__ distinct valleys\n",
    "\n",
    "- Either of which would be reported as \"correct\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, some ___Math facts___ to ease the pain\n",
    "\n",
    "- \"Gradient\" is just another word for rate of change\n",
    "  - In simple plots, this is just the slope.\n",
    "  - In other words\n",
    "    - Where a tensor supports values of n-dimensions, \n",
    "    - Gradients support rates of change in $(n-1)$-dimensions\n",
    "- Multi-variable calculus is encouraged because it does the standard (easy) calculations across an n-variable space\n",
    "  - Hurray for applied mathematics!\n",
    "- (Review) you can find the slope at a point one of two ways\n",
    "  - Infinitely approach it, from both sides, taking the slopes and approximating\n",
    "  - Taking the derivative, and computing for the given $x$-value\n",
    "    - $f(x) = x^2$\n",
    "    - $f'(x) = 2x$ (apply the power rule)\n",
    "    - We want the slope at $x = 2$, so just basic algebra:\n",
    "      - $f'(2) = 2(2) = 4$\n",
    "      - That's the slope/\"rate of change\"/gradient of the $x^2$ plotted parabola"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Mat says there's a way to avoid said valleys, [like momentum](http://sebastianruder.com/optimizing-gradient-descent/index.html#momentum) (as you roll through the well)\n",
    "\n",
    "- And this turns out to be [a pretty epic resource](http://sebastianruder.com/optimizing-gradient-descent/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent (returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "I can't help but think, \"why are we playing with perceptrons, when practically all the behavior we want will be archieved via a Sigmoid neuron?\"\n",
    "\n",
    "- Then I remind myself how Udacity doles this stuff out:\n",
    "  - \"Just enough to keep you going\"\n",
    "  - \"We'll give you the rest later. We swear\"\n",
    "\n",
    "Gotta keep my eye on the prize, and just keep going\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mat repeats a lot of what I covered in the \"Review\", with some additions\n",
    "\n",
    "In regards to the Sum of Squared Errors:\n",
    "\n",
    "1) Why not take the absolute value of the difference, instead of the square?\n",
    "> The square penalizes larger errors ... and makes the math nice later on\n",
    "\n",
    "2) Clarification of the activation function\n",
    "- Again, like I covered in the Review, it's simply the thing that determines the output of the node\n",
    "  - Takes into account the $bias$\n",
    "\n",
    "3) Trimmer definition of the SSE:\n",
    "\n",
    "\\begin{equation}\n",
    "E = \\frac 1 2 \\sum_{\\mu} (y^{\\mu} - \\hat{y}^{\\mu})^2\n",
    "\\end{equation}\n",
    "\n",
    "... and given than $\\hat{y} = \\sum_i w_i x_i^{\\mu}$ ...\n",
    "\n",
    "\\begin{equation}\n",
    "E = \\frac 1 2 \\sum_{\\mu} (y^{\\mu} - (\\sum_i w_i x_i^{\\mu}))^2\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Our data records__ are represented by $\\mu$\n",
    "\n",
    "- You can think of these as\n",
    "  - Two tables\n",
    "  - ... arrays\n",
    "  - ... matrices\n",
    "  - Whatever you want\n",
    "\n",
    "\n",
    "If it needed reiterating:\n",
    "\n",
    "1) The $\\sum_{\\mu}$ is just iterating the \"two tables\", summing up the SSE\n",
    "  - Two because you'll have your __inputs__ ($x$) and __targets__ ($y$)\n",
    "    - Gotta hit _some_ mark\n",
    "  - The SSE, because that's how we train - in this case\n",
    "\n",
    "2) (Like in the review) The SSE is a measure of how wrong we are\n",
    "  - If the SSE is high, we're making lots of bad predictions\n",
    "  - If it's low, we're on the road to good predictions\n",
    "    - We also don't want to change that much\n",
    "    - We've found the sweet spot, no need to move out of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Mat then asserts what I've derived:\n",
    "\n",
    "- With only one data record, there isn't a summation\n",
    "  - Because $[\\sum_{i=1}^1 f(x_i)] = f(1)$, for any $f(x)$\n",
    "- With only one output, there's only the inner summation\n",
    "  - And if we've only got one record, there's no $\\sum$ to speak of\n",
    "  - It's clear are rain\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before I get to the upcoming mic drop..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mat makes simple Gradient Descent\n",
    "\n",
    "Imagine a graph of $(7x - 8)^2 + 3$; a very thin parabola that doesn't have a *zero*\n",
    "\n",
    "- But its $y$-axis is labeled $E$, for our error\n",
    "- And its $x$-axis is labeled $w$, for our weight\n",
    "\n",
    "---\n",
    "Partial derivative seems to just be the derivative in one direction\n",
    "\n",
    "- Instead of the full slope, we only look at half of it\n",
    "  - Thus, a \"partial\"\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to *head towards the minimum*, which is the opposite of the slope\n",
    "\n",
    "- $\\Delta w = -gradient$\n",
    "\n",
    "> If we take increasingly small steps *down the gradient*, eventually, the weight will find the minimum of the error function\n",
    "- This is __Gradient Descent__\n",
    "\n",
    "Then we just redefine the weights:\n",
    "\n",
    "\\begin{equation}\n",
    "w_{i+1} = w_i + \\Delta w_i\n",
    "\\end{equation}\n",
    ", where \n",
    "\n",
    "- $\\Delta w_i$ is the determined step\n",
    "- $w_i$ is what used during the current iteration\n",
    "- $w_{i + 1}$ will be what we want to use next iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mat Speaks Math\n",
    "\n",
    "He derived a one line equation that represents the "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "2017년 4월 22일 - \n",
    "오후 10시 8분\n",
    "\n",
    "I'm going to try something that I haven't done since sophmore year of university:\n",
    "\n",
    "> Sophomore year of university, a professor of mine ran a \"MapReduce\" job on a MongoDB cluster and I just didn't get it.\n",
    "- I had yet to really be introduced to Functional Programming\n",
    "- I kept hearing the word, but I didn't connect the dots\n",
    "  - I'd just heard these words\n",
    "  - None of them had any real many to me at the time\n",
    "\n",
    "So what am I going to do?\n",
    "\n",
    "*I'll move on without 100%, cover-to-cover of some concept*\n",
    "\n",
    "I'm either going to take the AI (with Voice-UI concentration) Nanodegree or the Robotics nanodegree and I'm going to have __ample__ time to master these concepts\n",
    "\n",
    "- It's not irresponsible or life threatening to not know this stuff\n",
    "  - As it would be if I was in medicine\n",
    "- But to get on track I need to press past my perfectionist tendencies\n",
    "  - My cohort-mates are on week 4?\n",
    "  - I'm still finishing week 1?\n",
    "  - Not acceptable.\n",
    "\n",
    "So I'm going to \n",
    "\n",
    "1. Watch \"The Math\" once more\n",
    "2. Listen any questions that I want answered\n",
    "3. Address them\n",
    "  - Be that by learning the calculus myself (I do have three of the best books on the subject)\n",
    "  - Asking questions in the Slack\n",
    "  - Other research (i.e. \"An intuitive explanation of...\") via Bing or Google\n",
    "4. Move on.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My Questions\n",
    "\n",
    "In no intended order:\n",
    "\n",
    "- aoeuaoeuoaueoae\n",
    "\n",
    "...\n",
    "\n",
    "---\n",
    "I got off into the weeds for an hour - to study Calculus - when the mission is to write out my questions.\n",
    "\n",
    "I procrastinated by feigning like I was making progress.\n",
    "\n",
    "Really need to kick the habit. 오후 11시 40분이야!\n",
    "\n",
    "---\n",
    "\n",
    "- Can we only use SSE for error detection?\n",
    "- At which point did the math become \"nice later\"?\n",
    "  - ... as a result of us using the square, instead of the absolute value\n",
    "  - ~~Similarly, what niceties does halving in $\\frac 1 2 \\sum_{\\mu} ...$  afford us?~~\n",
    "    - Answered in \"Good to Know\"\n",
    "- Is the direction of our $\\Delta w$ (change in weights) *really* opposite the slope?\n",
    "  - If the slope is negative, don't we want to go down?\n",
    "  - This is probably just nit-picking semantics\n",
    "- What necessitates the \"__learning rate__\" $\\eta$ scaling, scalar component?\n",
    "  - It feels a lot like the way __bias__ was used\n",
    "- ~~Why rewrite $\\hat{y}$ as a function?~~\n",
    "  - Answered in \"good to know\"\n",
    "- Chain Rule\n",
    "\n",
    "\n",
    "### Good to know\n",
    "\n",
    "- With SSE, the Error of the system is a function of the __weights__\n",
    "  - So, if we adjust the weights well, we are indirectly adjusting the error of the system\n",
    "  - Aiming to minimize the error of course\n",
    "- $\\hat{y}$ is entirely dependent on the *weights* for that current iteration, so they rewrote it as a function\n",
    "  - \"Math reuse\"? ;)\n",
    "- The half is taken because\n",
    "  1. Reducing down the largeness of all those squares is a decent thing to do\n",
    "    - It is our own error factor\n",
    "    - Totally relevant and effectively subjective to the implementation\n",
    "  2. The Power Rule and Chain Rule\n",
    "    - Power Rule essentially takes the square from $(y-\\hat{y})^2$ and elimates the $\\frac 1 2$ from $\\frac 1 2 \\sum_{\\mu} ...$\n",
    "\n",
    "This whole video is actually really good, now that I've moved the calculus back into active memory\n",
    "\n",
    "- I don't just know that I know of the things\n",
    "  - This all actually makes sense, and I'm keeping up just fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "# Right away, the derivative of the sigmoid function looks interesting\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Inputs (aka \"input data\")\n",
    "x = np.array([0.1, 0.3])\n",
    "\n",
    "# Target\n",
    "y = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input-to-output weights\n",
    "weights = np.array([-0.8, 0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm really seeing a good use of Object-Oriented Design\n",
    "\n",
    "- These weights are directly associated with an input.\n",
    "- Some encapsulation a node would be nice\n",
    "  - With a \"Input port\" and a \"weight\" standing guard ;)\n",
    "    - Child-like imagination\n",
    "- At least an associated list or map... dang :P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# learning rate, eta in the weight step equation\n",
    "learnrate = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's a \"weight step equation\"?\n",
    "\n",
    "- Unless he's refering to the step in our process/algorithm for producing a weighted input?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The \"linear combination\" performed by the neuron (h in f(h) and f'(h))\n",
    "h = x.dot(weights) # hurray dot product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Can I just say, I think he means \"summation\" __every__ time he says \"linear combination\"?\n",
    "  - I've started training myself to hear \"summation\" every time does, just because that's what those symbols actually\n",
    "  - Like Elliot, from Mr. Robot, with \"Evil Corp\"\n",
    "- Unless this \"linear combination\" really is a category of mathematical operations which Summation belongs?\n",
    "  - I really can't say"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Network output\n",
    "nn_output = sigmoid(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This output is really $\\hat{y}$, since this is our single iteration\n",
    "\n",
    "- And it fuels our ability to put $y - \\hat{y}$ effectively in code as..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "error = y - nn_output # boom baby - Emperor's new Groove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_gradient = sigmoid_prime(h) # derivative inline (aka f'(h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the little delta in our fly-by math storm\n",
    "error_term = error * output_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gradient descent step\n",
    "del_w = learnrate * error_term * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ohhhhh. \"Step\" as in \"steps on a stair case\", right?\n",
    "\n",
    "- Need to double check this again [Ian's book](http://www.deeplearningbook.org/) and [Michael Nielson's book](http://neuralnetworksanddeeplearning.com/chap1.html#perceptrons)\n",
    "  - Best resources for my mind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "This section ends with a descent \"fill-in-the-blank\" quiz, which just shows the stlye of most machine learning engineers in python\n",
    "\n",
    "- It's mapped directly from the math, so it would make sense in that regard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using the [data from this UCLA study](http://www.ats.ucla.edu/stat/data/binary.csv) for this segment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleanup\n",
    "\n",
    "We should clean up the data, to fix categorical data that is conveyed as numbers\n",
    "\n",
    "- Applicants' schools are given ranks 1-4, but these aren't really numerical values\n",
    "> Rank 2 is not twice as much as rank 1, rank 3 is not 1.5 more than rank 2.\n",
    "\n",
    "- So we'll encode this data as a new column\n",
    "  - rank\\_{1-4}, with a bit to signify if the cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# so instead of a record like\n",
    "old_schema = {'gre': -1, 'gpa': 0.13, 'admitted': 0, 'rank': \"Rank 1\"}\n",
    "\n",
    "# we lay it out tabularly:\n",
    "# admit, GRE score, GPA, is_rank_{1-4}\n",
    "new_schema = np.array([0, -1, 0.13, 0, 0, 1, 0]) # so this guy from a rank 3 school"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: we've got standardized data in \"GPA\" and \"GRE score\" columns\n",
    "> which means to scale the values such they have zero mean and a standard deviation of 1.\n",
    "\n",
    "- This is necessary since the sigmoid squashes exceptionally small and large input values\n",
    "\n",
    "___We have to be careful___ about how we standardize, or else the training gradient performs poorly, dies off, and the network doesn't train.\n",
    "\n",
    "- Especially so if we don't standardize, in which case __weight initialization__ is particularly difficult for sustained training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Here's the code to clean the data as described above:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "admissions = pd.read_csv('binary.csv')\n",
    "\n",
    "# Make dummy variables for rank\n",
    "data = pd.concat([admissions, pd.get_dummies(admissions['rank'], prefix='rank')], axis=1)\n",
    "data = data.drop('rank', axis=1)\n",
    "\n",
    "# Standarize features\n",
    "for field in ['gre', 'gpa']:\n",
    "    mean, std = data[field].mean(), data[field].std()\n",
    "    data.loc[:,field] = (data[field]-mean)/std\n",
    "    \n",
    "# Split off random 10% of the data for testing\n",
    "np.random.seed(42)\n",
    "sample = np.random.choice(data.index, size=int(len(data)*0.9), replace=False)\n",
    "data, test_data = data.ix[sample], data.drop(sample)\n",
    "\n",
    "# Split into features and targets\n",
    "features, targets = data.drop('admit', axis=1), data['admit']\n",
    "features_test, targets_test = test_data.drop('admit', axis=1), test_data['admit']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Squared Error\n",
    "> Now that we're using a lot of data, summing up all the weight steps can lead to really large updates that make the gradient descent diverge. To compensate for this, you'd need to use a quite small learning rate.\n",
    "\n",
    "We just divide the sum (from the inner summation) by $m$-number of records to take the average.\n",
    "\n",
    "- This is all for the purpose of lower our learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Why bother with that learning rate scaling-scalar, if it's become an inconvenience?\n",
    "\n",
    "- Seems like a contrivance in the first place\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An equation (MSE)\n",
    "\n",
    "Error using *mean* instead of *sum*:\n",
    "\n",
    "\\begin{equation}\n",
    "E = \\frac {\\frac 1 2 \\sum_{\\mu} (y^{\\mu} - \\hat{y}^{\\mu})^2} m\n",
    "\\end{equation}\n",
    "\n",
    "But because $x/m = x \\frac 1 m$...\n",
    "\n",
    "\\begin{equation}\n",
    "E = \\frac 1 2m \\sum_{\\mu} (y^{\\mu} - \\hat{y}^{\\mu})^2\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ... and an algorithm\n",
    "\n",
    "... for updating weights with gradient descent\n",
    "\n",
    "1. Set the weight step to zero: $\\Delta w_i = 0$\n",
    "  - There's that \"step\", again. Now I'm certain he means that displacement on the curve\n",
    "2. For every record of data (this is a summation)\n",
    "  1. __Make a forward pass__ to calculate $\\hat{y}$, the output: $f(\\sum_i w_{i}x_{i})$\n",
    "    - This is the ___dot product___ when in 1-D space: $w \\cdot x$\n",
    "  2. __Calculate the error term__ for our output: $\\delta = (y - \\hat{y}) * f'(\\sum_i w_{i}x_{i})$\n",
    "    - Notice the first derivative of $\\hat{y}$'s equivalence\n",
    "  3. __Set the weight step__ for the __next__ pass: $\\Delta w_{iNext} = \\Delta w_i + \\delta x_i$\n",
    "    - That is *soooo* not math notation :P\n",
    "3. __Update the weights__, $w_{iNext} = w_i + \\eta {\\Delta w_i} / m$\n",
    "  - Where:\n",
    "    - $\\eta$ is the learning rate\n",
    "    - $m$ is the number of records\n",
    "  - We're averaging the weight steps to reduce any large variations in the training data\n",
    "    - Outliers get smeared into conforming\n",
    "4. __Repeat for $e$ epochs__\n",
    "  > An epoch is a single pass through the data. Iteration refers to processing a single batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "I can't tell if this is Stocastic or not, but I do know that I'll learn the details of [Gradient Descent with Khan Academy](https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/gradient-and-directional-derivatives/v/gradient) and [my textboox](http://a.co/4gAcGDu)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An alternative implementation\n",
    "\n",
    "> Updating the weights on each record instead of averaging the weight steps in-post\n",
    "\n",
    "Keep in mind:\n",
    "\n",
    "- Sigmoid activation, $f(h) = \\frac 1 {(1 + e^{-h})}$\n",
    "- Gradient (slope) of sigmoid, $f'(h) = f(h)(1 - f(h))$\n",
    "- And $h$ is the input to the output unit (activation function):\n",
    "  - $h = \\sum_i w_i x_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing with Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want the initial weights to be linearly-close to 0\n",
    "\n",
    "- We also want to minize __squashing__\n",
    "  - This \"squashing\" is different than the \"smearing\" I mentioned earlier.\n",
    "  - My intent was that we coerce them into aligning, and not being lost to the equations\n",
    "\n",
    "We initialize with a random normal distribution, so there's some kind of symmetry centered at 0\n",
    "\n",
    "- Scaling with $1/\\sqrt{n}$ keeps the parameter to the output function low for increasingly large inputs\n",
    "  - Where $n$ is the number of input units\n",
    "    - which I believe are just the number of nodes\n",
    "  - In other words, our solution scales with our data\n",
    "\n",
    "And to hear how Mat put all of this:\n",
    "\n",
    "> First, you'll need to initialize the weights. We want these to be small such that the input to the sigmoid is in the linear region near 0 and not squashed at the high and low ends. It's also important to initialize them randomly so that they all have different starting values and diverge, breaking symmetry. So, we'll initialize the weights from a normal distribution centered at 0. A good value for the scale is $1/\\sqrt{n}$ where $n$ is the number of input units. This keeps the input to the sigmoid low for increasing numbers of input units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "I'll put this inline, so it doesn't clutter the namespace later\n",
    "\n",
    "```python\n",
    "import random as rand\n",
    "import numpy as np\n",
    "\n",
    "# randomize for the sake of this example running\n",
    "n_features = rand.randint(1,10)\n",
    "\n",
    "# based off our description earlier\n",
    "weights = np.random.normal(scale=1/(n_features**.5), size=n_features)\n",
    "\n",
    "# output layer's parameter (called \"input\" just for the sake of confusing\n",
    "# those who aren't strictly mathematical)\n",
    "output_in = weights.dot(inputs) # hurray dot products!\n",
    "# This is also the h-value(?)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Mat kindly left off the in-code representations of __$\\Delta w_i$__, but this \"code example isn't supposed to run\"\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "He also pauses here to not that we can capture $f(h)$ because we re-use it in the Sigmoid's first-derivate.\n",
    "\n",
    "- Well duh?\n",
    "- I write good code, but this course is for the everyman - mostly\n",
    "  - Except you should have passed high school, and programmed Python before\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2017년 4월 23일 - 오전 9시 23분\n",
    "\n",
    "Nooow there's an implementation quiz (*data_prep* is the snippet above that cleans the data):\n",
    "\n",
    "__Completed__ 오전 9시 40분에\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from data_prep import features, targets, features_test, targets_test\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# TODO: We haven't provided the sigmoid_prime function like we did in\n",
    "#       the previous lesson to encourage you to come up with a more\n",
    "#       efficient solution. If you need a hint, check out the comments\n",
    "#       in solution.py from the previous lecture.\n",
    "\n",
    "# Use to same seed to make debugging easier\n",
    "np.random.seed(42)\n",
    "\n",
    "n_records, n_features = features.shape\n",
    "last_loss = None\n",
    "\n",
    "# Initialize weights\n",
    "weights = np.random.normal(scale=1 / n_features**.5, size=n_features)\n",
    "\n",
    "# Neural Network hyperparameters\n",
    "epochs = 1000\n",
    "learnrate = 0.5\n",
    "\n",
    "for e in range(epochs):\n",
    "    del_w = np.zeros(weights.shape)\n",
    "    for x, y in zip(features.values, targets):\n",
    "        # Loop through all records, x is the input, y is the target\n",
    "\n",
    "        # Note: We haven't included the h variable from the previous\n",
    "        #       lesson. You can add it if you want, or you can calculate\n",
    "        #       the h together with the output\n",
    "        h = x.dot(y)\n",
    "        # TODO: Calculate the output\n",
    "        output = sigmoid(h)\n",
    "\n",
    "        # TODO: Calculate the error\n",
    "        error = y - output\n",
    "\n",
    "        # TODO: Calculate the error term\n",
    "        error_term = error * (output * (1 - output)) # inlined sigmoid_prime\n",
    "\n",
    "        # TODO: Calculate the change in weights for this sample\n",
    "        #       and add it to the total weight change\n",
    "        del_w += learnrate * error_term * x\n",
    "\n",
    "    # TODO: Update weights using the learning rate and the average change in weights\n",
    "    weights += del_w\n",
    "\n",
    "    # Printing out the mean square error on the training set\n",
    "    if e % (epochs / 10) == 0:\n",
    "        out = sigmoid(np.dot(features, weights))\n",
    "        loss = np.mean((out - targets) ** 2)\n",
    "        if last_loss and last_loss < loss:\n",
    "            print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n",
    "        else:\n",
    "            print(\"Train loss: \", loss)\n",
    "        last_loss = loss\n",
    "\n",
    "\n",
    "# Calculate accuracy on test data\n",
    "tes_out = sigmoid(np.dot(features_test, weights))\n",
    "predictions = tes_out > 0.5\n",
    "accuracy = np.mean(predictions == targets_test)\n",
    "print(\"Prediction accuracy: {:.3f}\".format(accuracy))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "I've been successful in implementing the quizes, roughly, on the first try\n",
    "\n",
    "- On the instances where this wasn't the case, there was a syntax error\n",
    "\n",
    "This is decently inspiring, speaking frankly.\n",
    "\n",
    "- It's like, \"I got this. Why was I doubting?\"\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think this notebook needs to be split... It's getting ridiculously long.\n",
    "\n",
    "- Given that everything up to this point is related ...\n",
    "  - All the math\n",
    "  - The frustration\n",
    "  - The journaling\n",
    "- ... it would make sense to leave it all intact\n",
    "  - But the next section is actually getting into __hidden layers__\n",
    "    - __Deep__ly stacked hidden-layer in __learning__ neural __networks__\n",
    "  - And I __know__ that topic is big."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Continued in [Topic 3 Conclusion](./Topic 3 Conclusion - Multilayer Perceptrons to Conclusion.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
