{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using an example very similar to that of the \"Intro to Machine Learning\" lesson that covers *data surfaces*, we learn this process of determining whether a point *passes* or *fails* a tast (based on historics) is __Logistic Regression__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The example (with the similar data surface) is of student grades and entrance test scores, colored for acceptance\n",
    "  - green dots on the right hand of the D.S. are (supposedly) accepted\n",
    "  - red dots on the left hand are subsequently rejected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to think about this is: \"how easily can we separate the data?\" (with a line)\n",
    "- In this lesson, we'll gradually reduce our losses via __gradient decent__\n",
    "  - The *log-loss function* will have an output whose error we want to minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer this question:\n",
    "  - In which direction can we rotate (or move) the line for the maximum error reduction?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> It looks like we're proposing too many acceptances, because some of these students have low grades with good test scores...\n",
    "\n",
    "> Hmmmm.\n",
    "\n",
    "> Maybe a cirle?\n",
    "\n",
    "> How about two lines? Yes, __let's use two lines__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But how do we find the two lines?\n",
    "- Gradient descent still does the job.\n",
    "- We'll calculate against the log-loss function, looking for two different positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is a neural network\n",
    "1. Is this point over the horizontal line?\n",
    "2. Is this point over the vertical line?\n",
    "3. Are the answers to both 1 and 2 yes?\n",
    "  - This yields only a single \"yes\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This feels like the makings of a perceptron...\n",
    "- Doing bare-bones, basic boolean ANDs\n",
    "\n",
    "I'm wondering how we translate this truth-table-like problem into a system of nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then teach breaks it down for us ;)\n",
    "> Let's graph each question as a small node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A breakdown\n",
    "(To better form an intuition?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Is the point over the horizontal line?\n",
    "    - And two input nodes: test score and grades\n",
    "    - We can plot this like coordinate pair\n",
    "    - Outputs a Boolean\n",
    "2. Is the point over the vertical line?\n",
    "    - Ouputs a Boolean to answer the question\n",
    "3. Add a node (the __output node__), one layer \"in\"?\n",
    "    - This node receives the previous two inputs and logical ANDs them together\n",
    "    - Ouputs the result of that boolean addition(?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "But __how__ is this a neural network?\n",
    "\n",
    "Well just look at the layers like this:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{vmatrix}\n",
    "Test: & 1 \\\\\n",
    "Grade: & 8\n",
    "\\end{vmatrix} {_{->}^{->}}\n",
    "\\begin{vmatrix}\n",
    "horizLineTestNode \\\\\n",
    "vertLineTestNode\n",
    "\\end{vmatrix} {_{->}^{->}}\n",
    "\\begin{vmatrix}\n",
    "AND\n",
    "\\end{vmatrix} ->\n",
    "Output\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- picture the *test* and *grade* flowing into both test nodes (four arrows instead of two)\n",
    "- That's the general idea\n",
    "  - Data flows left-to-right\n",
    "  - Remember: a given neuron takes, as input, the output of other neurons\n",
    "    - We use numbers at this level\n",
    "    - But soon, we'll do so much more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I really don't want to type out a truth table, or a binary grid\n",
    "  - Please just remember this\n",
    "  - Or just review [this](http://kias.dyndns.org/comath/21.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Vocabulary:\n",
    "  - __Perceptrons__, or neurons, are individual nodes in an interconnected network\n",
    "    - They are the basic unit of neural networks\n",
    "    - Function: *Look at input data and decide how to categorize said data*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the previous example of school acceptance\n",
    "- inputs either passed the threshold for grades and test scores, or didn't\n",
    "- The outputs - \"yes\" and \"no\" - were the __categories__\n",
    "\n",
    "Those categories then combine to produce a decision!\n",
    "- Back example land, whether or not the student is granted admission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "But wait a second....\n",
    "\n",
    "How the heck do these nodes know whats important in checking that threshold?\n",
    "> When initialized, we don't know what information will be most important.\n",
    "\n",
    "> So... We have the network __learn for itself__. And even let it adjust how it considers the data.\n",
    "\n",
    "All of this is done with a little something called..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "My turn.\n",
    "\n",
    "Perceptrons (aka \"neurons\", \"nodes\") have weights for all the inputs that they receive\n",
    "- Think of this like a person who values certain peoples' opinions more than others\n",
    "\n",
    "Based on the inputs (and associated weights), an output can be determined.\n",
    "- And since neurons will only be connected to a set number of other neurons, it is safe to have a fixed number of weights!\n",
    "  - It all makes sense in my brain: \n",
    "    - each \"opinion\" has more/less sway on the node, in its categorization\n",
    "  - Let's see how the course explains it.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> When input data is received, it get multiplied by a weight that is assigned to this particular input.\n",
    "- If we have our example of school acceptance\n",
    "  - `tests` could be an input name for test scores\n",
    "  - `grades` could be an input name for the student's average grades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Pretty much what I said, with a different tone of voice, and vocabulary.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note__ that neuron's given weight for some input starts out random (adding up to 1.00? because percentages???)\n",
    "- Overtime, as the neural network learns more about the kind of input data that leads to student acceptance\n",
    "  - The network adjusts weights based on erors in categorization (from previous results)\n",
    "  - This is called __training__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabulary:\n",
    "  - __training__: The process of improving a network's individual node-level weights, to produce the desired output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- An __extreme__ example of weights would be if the test scores had *no affect at all* on the university acceptance\n",
    "  - The weight of \"test score\"-input would be zero and have no affect on the output of the perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Input (at the perceptron level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to review:\n",
    "\n",
    "- Each input has a weight that represents its importance\n",
    "- Weights are determined during the learning process (aka, __training__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summing the Input Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note that weights will *always* be represented by some type of the letter __w__. It will be capitalized - __W__ - when it represents a __matrix__\n",
    "- Subscript will specify *which* weights.\n",
    "\n",
    "Just remember the variable naming conventions: both math and code\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "They set this equation as the relationship between weights and inputs:\n",
    "\n",
    "\\begin{equation}\n",
    "w_{grades} \\cdot x_{grades} + w_{test} \\cdot x_{test} = -1 \\cdot x_{grades} - 0.2 \\cdot x_{test}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The perceptron applies these weights to the inputs and sums them in a __linear combination__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The next section is verbose as it tries to skirt around the math notation - it's just summations\n",
    "- Of `x_input` and `w_input`, which is the __input__ and the __weight__ associated with said __input__ \n",
    "  - `x` differentiates it, nothing more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Summatively, that looks like:\n",
    "\n",
    "\\begin{equation}\n",
    "\\sum_{i=1}^m w_{i} \\cdot x_{i}\n",
    "\\end{equation}\n",
    "\n",
    "- Where *m* is the number of inputs\n",
    "  - This can be omitted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Please Note\n",
    "that the relative size of weights is what's most important, not the weights themselves\n",
    "\n",
    "- He uses an example of\n",
    "  - w_grades = -1\n",
    "  - w_test = -.2\n",
    "- And points out that the ratio of 5:1 is what's important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__In brief__, multiply an input by its weight\n",
    "\n",
    "- Take an opinion by its importance factor\n",
    "- It's that easy.\n",
    "  - Now to learn about these activation functions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Output: an Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First the analogy that makes the most since in my mind:\n",
    "> Consider an actual neuron in the brain:\n",
    "- It absorbs charge from its neighbors\n",
    "- After a certain amount of input, it discharges\n",
    "\n",
    "> When the activation function fires, the neuron discharges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__He shows__ an activation function as mearly a conditional function:\n",
    "\n",
    "\\begin{equation}\n",
    "f(h) = \\{_{1 if h \\geq 0}^{0 if h \\lt 0}\n",
    "\\end{equation}\n",
    "\n",
    "- Apparently, this function is called the __Heaviside Step Function__\n",
    "  - This sucks for university acceptance :P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "This guy is really making this seem too easy.\n",
    "- I get that the math is simple, but his round-about way of explaining it is frustrating\n",
    "  - I know how to graph on a chart!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we've written a dodgy function (only poor students activate). We'll fix it\n",
    "- The instructor's finally getting to the point!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias, or \"shifting the goods\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have a good shape (to the graph of acceptable inputs), but we don't like the selection.\n",
    "- Just add __bias__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Vocabulary:\n",
    "\n",
    "  - __bias__ is a translation of the graph of acceptable inputs.\n",
    "    - Simple *adding* to the result of the base equation\n",
    "    - First demonstrated on the y-axis.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "For feedback:\n",
    "\n",
    "> This feels like a bad way to explain this. Sure, not everyone remembers mathematics as I do. But this doesn't do the equations justice, and I think it would be inappropriate for students to feel confident with these hand-wave-y definitions\n",
    "\n",
    "- The math is clear, but the words are __not__.\n",
    "  - Almost has me doubting my math\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More to my point, I'm just going to copy the instructors paragraph..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Note to self:\n",
    "\n",
    "- I keep slipping in and out of the Feyman techinque.\n",
    "- I want to rearticulate it, but there's nothing to say\n",
    "  - __ADD__ a bias. Literally.\n",
    "    - Some number, by the science of neural networks, will be generated to shift the good shape to have the right coverage\n",
    "  - I'm not going to re-explain [what summation is](https://en.wikipedia.org/wiki/Summation)\n",
    "    - I've been familiar with this concept for longer than I've been programming.\n",
    "    - Here's my attempt at it:\n",
    "      - For every *single* number between \\*points to start and end of the summation\\*\n",
    "      - Solve the equation with __that__ number as a result\n",
    "      - Add __all__ of those results together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![perceptron formula](https://d17h27t6h515a5.cloudfront.net/topher/2017/February/58951180_perceptron-equation-2/perceptron-equation-2.gif \"Perceptron Formula\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the aformentioned, poor, presumptive summary:\n",
    "\n",
    "> This formula returns $1$ if the input ($x_{1}$,$x_{2}$,...,$x_{m}$) belongs to the accepted-to-university category or returns 0 if it doesn't. The input is made up of one or more real numbers, each one represented by $x_i$, where $m$ is the number of inputs.\n",
    "\n",
    "Then the neural network starts to learn! Initially, the weights ( $w_{i}$ ) and bias ( $b$ ) are assigned a random value, and then they are updated using a learning algorithm like gradient descent. The weights and biases change so that the next training example is more accurately categorized, and patterns in data are \"learned\" by the neural network.\n",
    "\n",
    "Now that you have a good understanding of perceptions, let's put that knowledge to use. In the next section, you'll create the AND perceptron from the Neural Networks video by setting the values for weights and bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While I feel like I'm growing comfortable with Neural Networks, I'm seriously disappointed in this section\n",
    "- \"Uhhh... We can't figure out how to teach this to you, so we're going to give you an impartial __text__ lecture of that is heavy on mathematics, but without the explanations of their necessity\"\n",
    "- It's like Siraj's videos, but less goofy; more stoic\n",
    "  - That is __not__ a good thing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quizzes, or \"suffering will be your teacher\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "This first quiz feels almost unfair, given the abyssmal wrap up of the last section.\n",
    "\n",
    "- I'm about to read another resource to get together a more decent understanding of the activation.\n",
    "  - I can appreciate the abiguity of \"you define it however you want\", but what it this threshold for effective\n",
    "    - How could you ever know before hand?\n",
    "  - I don't imagine the \"Heaviside step function\" will always suffice for an activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a \"fill in the blank\", with the weights of the inputs, a bunch of graphs showing desired behavior, and frustration...\n",
    "- They want the AND function first..\n",
    "  - It's contrived instruction, to get us used to using the $b$\n",
    "- `AND` can be programmed much more simply."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "So, some how, changing the weights and bias for these logic gate neural networks is relaxing my mind.\n",
    "\n",
    "- My brain is just treating them like puzzles.\n",
    "  - I still don't know if this is a good thing\n",
    "- After I gave the answer *it wanted* for \"how would we shift this graph to go from AND to OR?\"\n",
    "  - For the record: \"Increase the weights\" and \"Decrease the magnitude of the bias\"\n",
    "\n",
    "__ALL__ of these are still using the aforementioned biased step function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "Back at it:\n",
    "> A neural network is like any tool. You have to know when to use it.\n",
    "\n",
    "Thankful they didn't make me play the \"weight puzzle game\" with XOR - it's always a pain.\n",
    "- Thankfully, the XOR is a composition of\n",
    "  - two NOT gates\n",
    "  - two AND gates\n",
    "  - one OR gate\n",
    "  - a few passthroughs to give the illusion of \"layers\".\n",
    "- It's __very__ similar to the circuit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The power of a neural network isn't building it by hand, like we were doing. It's the ability to learn from examples. In the next few sections, you'll learn how a neural networks sets it's own weights and biases.\n",
    "\n",
    "... Yeah. Hopefully.\n",
    "- I feel like I've learned a lot of vocabulary, and a few theoretical applications of math I've known for ages...\n",
    "  - And refreshed my Linear Algebra, which is always good.\n",
    "- I've seen glimpses of the applications of that Matrix Math in these neural systems\n",
    "  - I want to weild that power...\n",
    "  - There, I said it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mat returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "He gives a breakdown of all that goes into a neuron:\n",
    "- Inputs\n",
    "  - And associated weights\n",
    "- A bias\n",
    "- resultant input, $h$\n",
    "- Activation function $f(h)$\n",
    "- Output, $y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drawn as such:\n",
    "\n",
    "![image](https://d17h27t6h515a5.cloudfront.net/topher/2017/February/589366f0_simple-neuron/simple-neuron.png \"Cirles are units, boxes are operations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is rather decent, leading to some insights:\n",
    "\n",
    "- Construction of the activation function argument\n",
    "  - Weighted input, with bias\n",
    "  - In other words: $x_1 \\cdot w_1 + b$\n",
    "- The argument to the activation function: $h$\n",
    "- The exit - and output - is __activation function__: $y = f(h)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "My mind understands composition.\n",
    "\n",
    "This a layer, a composition, of trivial operations\n",
    "\n",
    "- Multiplication\n",
    "- Addition\n",
    "- Identity\n",
    "\n",
    "... Along with some arbitrary activation function.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mat has us program the [Sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function) and they keep repeating this:\n",
    "\n",
    "> stacking units will let you model linearly inseparable data, impossible to do with regression models.\n",
    "\n",
    "This phrase \"linearly inseperable data\" has been said four times now.\n",
    "- I haven't heard it before\n",
    "- But I don't think many operations a above decomposition.\n",
    "  - I actually think there are infinitely many operations which can be decomposed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "I'm not going to run the code, but here it is:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    # TODO: Implement sigmoid function\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "inputs = np.array([0.7, -0.3])\n",
    "weights = np.array([0.1, 0.8])\n",
    "bias = -0.1\n",
    "\n",
    "# TODO: Calculate the output\n",
    "output = sigmoid(np.dot(weights, inputs) + bias)\n",
    "\n",
    "print('Output:', output)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "## Learning Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we don't know the correct weights, we have to learn them from history.\n",
    "\n",
    "- Example data\n",
    "\n",
    "Then we can then predict (produce outputs) with those weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Know when you're wrong\n",
    "\n",
    "Since we start with random weights, we need to adjust those weights towards what is proper\n",
    "\n",
    "- *iterate*, if you will"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__So we *measure*__ how wrong we are, to gather some sense of how to course correct\n",
    "\n",
    "- i.e. \"Too big\", \"too big\", \"too hot\", \"too cold\", \"just right\"\n",
    "  - Thanks little Red\n",
    "\n",
    "One of the best methods for doing this is the \"[Sum of Squared Errors](https://en.wikipedia.org/wiki/Residual_sum_of_squares)\":\n",
    "\n",
    "\\begin{equation}\n",
    "E = \\frac 1 2 \\sum{_\\mu} \\sum{_j} [y_j^\\mu - \\hat{y}_j^\\mu]^2\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's break it down\n",
    "\n",
    "Pseudo-academia (me):\n",
    "> Take half of the sum of the sum of:\n",
    "  - all outputs (predictions) subtracted from the true value for some given data point $\\mu$ and a predction $j$ squared\n",
    "\n",
    "Layman's terms:\n",
    "> For every data point (in our training set) calculate the inner sum of \n",
    "- The squared difference of\n",
    "- The true value (categorization [think back]) of a data point, and the prediction for that data point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "We're building a graph and we need to get the shape right.\n",
    "- The squaring is probably inherited from statistical models that fit to a curve. \n",
    "  - Or correlation against a best fit line\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "오후 11시 9분 끝났습니다...\n",
    "\n",
    "Having studied hard, had an awful work day, and endured ill-suited-for-me instruction (\"intellectual babble\", that uses all the right words sans substance), I just want to go to sleep.\n",
    "- My chest is heavy.\n",
    "  - It hurts to feel this much angst\n",
    "  - It's disappointing that getting started is such a labor\n",
    "- Was this really worth \\$600?\n",
    "  - When I've got so many things I could have read for free\\?\n",
    "  - When the, arguably, most important lesson in this course isn't singing true like the previous math lesson\\?\n",
    "    - 왜? 어떡해? :("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2017년 4월 20일 - Reviewing\n",
    "-\n",
    "\n",
    "First off: yes, it is worth it.\n",
    "\n",
    "While the teaching may not be my style - \"look what we can do. And here's some math... Ooo! Shiny, cool things\" - there is genuinely good content here.\n",
    "\n",
    "- The lesson on matrices and Linear Algebra is learning a semester's worth of work in a few hours.\n",
    "  - Concise and effective\n",
    "- Mat's visual-driven lessons are actually effective, which I'm grateful for.\n",
    "  - Human brains are naturally potent at visual comprehension\n",
    "    - Which is why teaching a computer to see is so dang impressive\n",
    "\n",
    "What I need is patience with this, and do what I do best:\n",
    "\n",
    "- Put it together in my head, in a way that makes sense to me.\n",
    "  - This curriculum is designed to hit for everyone\n",
    "  - Not like [some courses](http://neuralnetworksanddeeplearning.com/chap1.html#perceptrons), which cater to 내 스타일"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Let's talk about that other \"course\" (it's a book) for a moment.\n",
    "\n",
    "In about 10 minutes of reading, he has addressed almost __everything__ that this Udacity course has waved its hands at\n",
    "\n",
    "- Node: perceptron and Sigmoid neuron are __different__\n",
    "  - Perceptron does binary output\n",
    "  - Sigmoid neuron outputs the result of a Sigmoid against some input\n",
    "- Gradient descent\n",
    "  - He doesn't refer to it like we already know what it is\n",
    "  - Instead, he talks about the math and it's utility.\n",
    "- Others that I (hopefully) will fill out later.\n",
    "\n",
    "At work, I tried to employ the Feynman technique to teach all that I know about Neural Networks.\n",
    "\n",
    "- I will attempt to do so again here...\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks (Feynman self-Review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A neural network is a system of interconnected (specialized) neurons, organized in columns we call __layers__.\n",
    "\n",
    "- The two most common types of neuron (henceforth, \"node\") are:\n",
    "  - Perceptron, which outputs binary (typically via some Heaviside Step function)\n",
    "  - Sigmoid neuron, which outputs the result of a Sigmoid (see above) function\n",
    "\n",
    "__Perceptrons__, thanks to their binary nature, can parallel any logical operation.\n",
    "\n",
    "- They can become an AND, NAND, OR, or XOR gate\n",
    "  - Parallel to circuitry, and equally composable\n",
    "- Because this logical similarity, neural networks can perform any computation\n",
    "  - Treating the nodes as logic gates\n",
    "    - In-memory circuitry, if you will\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Hypothesis: This proved to be really slow, which is why we didn't get very far with neural network-based Machine Learning for decades.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Perceptron) Nodes take input, and __categorize__ it: determine it exceeds a threshold\n",
    "\n",
    "- That \"determining\" process is just a '$\\geq$' comparison; nothing complicated\n",
    "- Nodes treat their input as humans take opinion's\n",
    "  - \"With a grain of salt\"\n",
    "  - A unique weight factor (\"factor\", think \"multiplication\") is applied to each input\n",
    "    - $x_{input1}$, is the actual input\n",
    "      - Mathematicians love this variable. It's so easy to write ;)\n",
    "    - $w_{input1}$, is the weight associated with it\n",
    "    - Let's use an intermediary $h$ to represent that product of $x_{input1} * w_{input1}$\n",
    "  - The result, $h$, and the some (optional) bias $b$ are added together\n",
    "- Now __we finally have__ something number to compare\n",
    "  - $h + b \\ge 0$? output $1$\n",
    "  - otherwise, output $0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A human analogy:\n",
    "\n",
    "- I think Bob is a bit of a tosspot, so I don't really value what he says\n",
    "  - \"I'm only going to pay partial attention when he's talking to me\"\n",
    "- However, Jenny has been my crush for 3 years, so she has more sway\n",
    "  - \"She might say she'll date me, so I'm going to listen hard\"\n",
    "\n",
    "\n",
    "Bob's *input* is undervalued __relative__ to Jenny. That's the extent of inter-input-relation\n",
    "\n",
    "- One input means nothing to another"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "This gets marginally more complicated with more inputs\n",
    "\n",
    "- Most networks have multiple inputs, which are all processed by all nodes\n",
    "- In this case, we take the summatino of all our \"$h + b$\"-s\n",
    "  - We usually write it in the long form as: $\\sum_{i=1}{(x_i * w_i) + b}$\n",
    "    - Where $i$ in the equation $x_i * w_i$ is a give input value in a list of inputs values\n",
    "      - Remember, we've got multiple inputs here\n",
    "      - With programming, this would be like an array index\n",
    "    - We swap $h$ back out for what it's equal to\n",
    "  - This can also be expressed as a dot product\n",
    "    - $weights = [1, 1, 3]$.\n",
    "      - Let's say these are Bob, Joe, and Jenny's opinions\n",
    "    - $inputs = [5, 6, 3]$\n",
    "      - Let's say these are the number of words in their responses\n",
    "        - And a certain three-word sentence would make someone's life\n",
    "    - $bias = 3$, in a \"learning mood\"\n",
    "    - $weights \\cdot inputs = 1*5 + 1*6 + 3*3$ = 20\n",
    "  - Now we just test that $20$ against our threshold, and we've got output!\n",
    "- That's all there is to calculation.\n",
    "  - The funny part is when we get to *calculating weights on our own*\n",
    "  - No foreknowledge.\n",
    "    - Just your wits about you\n",
    "    - And math ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__There are an infinitely many__ number of ways to approximate appropriate weights.\n",
    "\n",
    "BUT BEFORE ANYTHING ELSE, you have to have effective data to learn from. Otherwise, we're just going crazy here.\n",
    "\n",
    "Here is what I know:\n",
    "\n",
    "1. Our guesses are initially random\n",
    "2. They are wrong to *some degree*\n",
    "3. We must correct against the wrongness\n",
    "\n",
    "\n",
    "Let's look at each of these in turn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1. Initially random__\n",
    "\n",
    "- We really just pick a number\n",
    "  - The idea is \"you gotta start somewhere\"\n",
    "- Very simple to implement programmatically\n",
    "```python\n",
    "import random as rand\n",
    "print(rand.randint(0,10))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2. Our guess is wrong *to some degree*__\n",
    "\n",
    "It would be amazing if our guess were always right.\n",
    "\n",
    "- Heck, that's practically what we're trying to build here, with the network\n",
    "\n",
    "But, it is very likely that our guess is __not__ right.\n",
    "\n",
    "To measure how wrong we are several calculations can be employed.\n",
    "\n",
    "One such calculation is the \"Sum of Squared Errors\", which is really a sum of sums, but that's over complicating the matter.\n",
    "\n",
    "1. We take the difference of our guessed output (for a given input) and the \"true\" output\n",
    "  - $y - \\hat{y}$, where $y$ is the true output and $\\hat{y}$ is a guess\n",
    "2. Square it\n",
    "  - Why?\n",
    "    - So we can have __a.__ a positive number to work with and __b.__ accentuate big differences\n",
    "3. Do this for every output node of the network, then sum them\n",
    "  - Yes, there can be multiple output nodes\n",
    "    - Not ___really___ in perceptrons\n",
    "  - This is that first sum\n",
    "4. Do this for all input-weight pairs (which should come from a bunch of data points), and __sum__ them\n",
    "  - This is that second summation\n",
    "\n",
    "And here's the crazy math equation, where $E$ is \"error\":\n",
    "\n",
    "\\begin{equation}\n",
    "E = \\frac 1 2 \\sum_{\\mu} \\sum_j [y_j^{\\mu} - \\hat{y}_j^{\\mu}]^2\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "__Think about how this relates__ to the perceptron equations, and it actually makes sense:\n",
    "\n",
    "A perceptron only has __1__ output node, so we only did that summation ___once___.\n",
    "\n",
    "- If there had been more output nodes, we would have done it those $m$ times!\n",
    "\n",
    "Eureka\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3. We must correct against our wrongness__\n",
    "\n",
    "One of the most common methods of correction is (stocastic) __Gradient Descent__\n",
    "\n",
    "- I haven't learned this yet, but I do know a few things about it\n",
    "\n",
    "Namely:\n",
    "\n",
    "- It effectively test extremes in a \"too hot, too cold\" manner\n",
    "  - It hops side-to-side, downwards towards the bottom of the \"well\" of this curve\n",
    "  - Every time it jumps, it decreases the intensity, so as to not jump *strictly* side-to-side\n",
    "- It is sensitive to local minima:\n",
    "  - ![caveat local minima](https://d17h27t6h515a5.cloudfront.net/topher/2017/January/587c5ebd_local-minima/local-minima.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determining which is \"correct\" is difficult from this image, but there are __two__ distinct valleys\n",
    "\n",
    "- Either of which would be reported as \"correct\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, some ___Math facts___ to ease the pain\n",
    "\n",
    "- \"Gradient\" is just another word for rate of change\n",
    "  - In simple plots, this is just the slope.\n",
    "  - In other words\n",
    "    - Where a tensor supports values of n-dimensions, \n",
    "    - Gradients support rates of change in $(n-1)$-dimensions\n",
    "- Multi-variable calculus is encouraged because it does the standard (easy) calculations across an n-variable space\n",
    "  - Hurray for applied mathematics!\n",
    "- (Review) you can find the slope at a point one of two ways\n",
    "  - Infinitely approach it, from both sides, taking the slopes and approximating\n",
    "  - Taking the derivative, and computing for the given $x$-value\n",
    "    - $f(x) = x^2$\n",
    "    - $f'(x) = 2x$ (apply the power rule)\n",
    "    - We want the slope at $x = 2$, so just basic algebra:\n",
    "      - $f'(2) = 2(2) = 4$\n",
    "      - That's the slope/\"rate of change\"/gradient of the $x^2$ plotted parabola"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Mat says there's a way to avoid said valleys, [like momentum](http://sebastianruder.com/optimizing-gradient-descent/index.html#momentum) (as you roll through the well)\n",
    "\n",
    "- And this turns out to be [a pretty epic resource](http://sebastianruder.com/optimizing-gradient-descent/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent (returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "I can't help but think, \"why are we playing with perceptrons, when practically all the behavior we want will be archieved via a Sigmoid neuron?\"\n",
    "\n",
    "- Then I remind myself how Udacity doles this stuff out:\n",
    "  - \"Just enough to keep you going\"\n",
    "  - \"We'll give you the rest later. We swear\"\n",
    "\n",
    "Gotta keep my eye on the prize, and just keep going\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mat repeats a lot of what I covered in the \"Review\", with some additions\n",
    "\n",
    "In regards to the Sum of Squared Errors:\n",
    "\n",
    "1) Why not take the absolute value of the difference, instead of the square?\n",
    "> The square penalizes larger errors ... and makes the math nice later on\n",
    "\n",
    "2) Clarification of the activation function\n",
    "- Again, like I covered in the Review, it's simply the thing that determines the output of the node\n",
    "  - Takes into account the $bias$\n",
    "\n",
    "3) Trimmer definition of the SSE:\n",
    "\n",
    "\\begin{equation}\n",
    "E = \\frac 1 2 \\sum_{\\mu} (y^{\\mu} - \\hat{y}^{\\mu})^2\n",
    "\\end{equation}\n",
    "\n",
    "... and given than $\\hat{y} = \\sum_i w_i x_i^{\\mu}$ ...\n",
    "\n",
    "\\begin{equation}\n",
    "E = \\frac 1 2 \\sum_{\\mu} (y^{\\mu} - (\\sum_i w_i x_i^{\\mu}))^2\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Our data records__ are represented by $\\mu$\n",
    "\n",
    "- You can think of these as\n",
    "  - Two tables\n",
    "  - ... arrays\n",
    "  - ... matrices\n",
    "  - Whatever you want\n",
    "\n",
    "\n",
    "If it needed reiterating:\n",
    "\n",
    "1) The $\\sum_{\\mu}$ is just iterating the \"two tables\", summing up the SSE\n",
    "  - Two because you'll have your __inputs__ ($x$) and __targets__ ($y$)\n",
    "    - Gotta hit _some_ mark\n",
    "  - The SSE, because that's how we train - in this case\n",
    "\n",
    "2) (Like in the review) The SSE is a measure of how wrong we are\n",
    "  - If the SSE is high, we're making lots of bad predictions\n",
    "  - If it's low, we're on the road to good predictions\n",
    "    - We also don't want to change that much\n",
    "    - We've found the sweet spot, no need to move out of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Mat then asserts what I've derived:\n",
    "\n",
    "- With only one data record, there isn't a summation\n",
    "  - Because $[\\sum_{i=1}^1 f(x_i)] = f(1)$, for any $f(x)$\n",
    "- With only one output, there's only the inner summation\n",
    "  - And if we've only got one record, there's no $\\sum$ to speak of\n",
    "  - It's clear are rain\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before I get to the upcoming mic drop..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mat makes simple Gradient Descent\n",
    "\n",
    "Imagine a graph of $(7x - 8)^2 + 3$; a very thin parabola that doesn't have a *zero*\n",
    "\n",
    "- But its $y$-axis is labeled $E$, for our error\n",
    "- And its $x$-axis is labeled $w$, for our weight\n",
    "\n",
    "---\n",
    "Partial derivative seems to just be the derivative in one direction\n",
    "\n",
    "- Instead of the full slope, we only look at half of it\n",
    "  - Thus, a \"partial\"\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to *head towards the minimum*, which is the opposite of the slope\n",
    "\n",
    "- $\\Delta w = -gradient$\n",
    "\n",
    "> If we take increasingly small steps *down the gradient*, eventually, the weight will find the minimum of the error function\n",
    "- This is __Gradient Descent__\n",
    "\n",
    "Then we just redefine the weights:\n",
    "\n",
    "\\begin{equation}\n",
    "w_{i+1} = w_i + \\Delta w_i\n",
    "\\end{equation}\n",
    ", where \n",
    "\n",
    "- $\\Delta w_i$ is the determined step\n",
    "- $w_i$ is what used during the current iteration\n",
    "- $w_{i + 1}$ will be what we want to use next iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mat Speaks Math\n",
    "\n",
    "He derived a one line equation that represents the "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayered Perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropogation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
