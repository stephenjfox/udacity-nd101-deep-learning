{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using an example very similar to that of the \"Intro to Machine Learning\" lesson that covers *data surfaces*, we learn this process of determining whether a point *passes* or *fails* a tast (based on historics) is __Logistic Regression__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The example (with the similar data surface) is of student grades and entrance test scores, colored for acceptance\n",
    "  - green dots on the right hand of the D.S. are (supposedly) accepted\n",
    "  - red dots on the left hand are subsequently rejected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to think about this is: \"how easily can we separate the data?\" (with a line)\n",
    "- In this lesson, we'll gradually reduce our losses via __gradient decent__\n",
    "  - The *log-loss function* will have an output whose error we want to minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer this question:\n",
    "  - In which direction can we rotate (or move) the line for the maximum error reduction?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> It looks like we're proposing too many acceptances, because some of these students have low grades with good test scores...\n",
    "\n",
    "> Hmmmm.\n",
    "\n",
    "> Maybe a cirle?\n",
    "\n",
    "> How about two lines? Yes, __let's use two lines__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But how do we find the two lines?\n",
    "- Gradient descent still does the job.\n",
    "- We'll calculate against the log-loss function, looking for two different positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is a neural network\n",
    "1. Is this point over the horizontal line?\n",
    "2. Is this point over the vertical line?\n",
    "3. Are the answers to both 1 and 2 yes?\n",
    "  - This yields only a single \"yes\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This feels like the makings of a perceptron...\n",
    "- Doing bare-bones, basic boolean ANDs\n",
    "\n",
    "I'm wondering how we translate this truth-table-like problem into a system of nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then teach breaks it down for us ;)\n",
    "> Let's graph each question as a small node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A breakdown\n",
    "(To better form an intuition?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Is the point over the horizontal line?\n",
    "    - And two input nodes: test score and grades\n",
    "    - We can plot this like coordinate pair\n",
    "    - Outputs a Boolean\n",
    "2. Is the point over the vertical line?\n",
    "    - Ouputs a Boolean to answer the question\n",
    "3. Add a node (the __output node__), one layer \"in\"?\n",
    "    - This node receives the previous two inputs and logical ANDs them together\n",
    "    - Ouputs the result of that boolean addition(?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "But __how__ is this a neural network?\n",
    "\n",
    "Well just look at the layers like this:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{vmatrix}\n",
    "Test: & 1 \\\\\n",
    "Grade: & 8\n",
    "\\end{vmatrix} {_{->}^{->}}\n",
    "\\begin{vmatrix}\n",
    "horizLineTestNode \\\\\n",
    "vertLineTestNode\n",
    "\\end{vmatrix} {_{->}^{->}}\n",
    "\\begin{vmatrix}\n",
    "AND\n",
    "\\end{vmatrix} ->\n",
    "Output\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- picture the *test* and *grade* flowing into both test nodes (four arrows instead of two)\n",
    "- That's the general idea\n",
    "  - Data flows left-to-right\n",
    "  - Remember: a given neuron takes, as input, the output of other neurons\n",
    "    - We use numbers at this level\n",
    "    - But soon, we'll do so much more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I really don't want to type out a truth table, or a binary grid\n",
    "  - Please just remember this\n",
    "  - Or just review [this](http://kias.dyndns.org/comath/21.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Vocabulary:\n",
    "  - __Perceptrons__, or neurons, are individual nodes in an interconnected network\n",
    "    - They are the basic unit of neural networks\n",
    "    - Function: *Look at input data and decide how to categorize said data*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the previous example of school acceptance\n",
    "- inputs either passed the threshold for grades and test scores, or didn't\n",
    "- The outputs - \"yes\" and \"no\" - were the __categories__\n",
    "\n",
    "Those categories then combine to produce a decision!\n",
    "- Back example land, whether or not the student is granted admission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "But wait a second....\n",
    "\n",
    "How the heck do these nodes know whats important in checking that threshold?\n",
    "> When initialized, we don't know what information will be most important.\n",
    "\n",
    "> So... We have the network __learn for itself__. And even let it adjust how it considers the data.\n",
    "\n",
    "All of this is done with a little something called..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "My turn.\n",
    "\n",
    "Perceptrons (aka \"neurons\", \"nodes\") have weights for all the inputs that they receive\n",
    "- Think of this like a person who values certain peoples' opinions more than others\n",
    "\n",
    "Based on the inputs (and associated weights), an output can be determined.\n",
    "- And since neurons will only be connected to a set number of other neurons, it is safe to have a fixed number of weights!\n",
    "  - It all makes sense in my brain: \n",
    "    - each \"opinion\" has more/less sway on the node, in its categorization\n",
    "  - Let's see how the course explains it.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> When input data is received, it get multiplied by a weight that is assigned to this particular input.\n",
    "- If we have our example of school acceptance\n",
    "  - `tests` could be an input name for test scores\n",
    "  - `grades` could be an input name for the student's average grades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Pretty much what I said, with a different tone of voice, and vocabulary.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note__ that neuron's given weight for some input starts out random (adding up to 1.00? because percentages???)\n",
    "- Overtime, as the neural network learns more about the kind of input data that leads to student acceptance\n",
    "  - The network adjusts weights based on erors in categorization (from previous results)\n",
    "  - This is called __training__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabulary:\n",
    "  - __training__: The process of improving a network's individual node-level weights, to produce the desired output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- An __extreme__ example of weights would be if the test scores had *no affect at all* on the university acceptance\n",
    "  - The weight of \"test score\"-input would be zero and have no affect on the output of the perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Input (at the perceptron level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to review:\n",
    "\n",
    "- Each input has a weight that represents its importance\n",
    "- Weights are determined during the learning process (aka, __training__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summing the Input Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note that weights will *always* be represented by some type of the letter __w__. It will be capitalized - __W__ - when it represents a __matrix__\n",
    "- Subscript will specify *which* weights.\n",
    "\n",
    "Just remember the variable naming conventions: both math and code\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "They set this equation as the relationship between weights and inputs:\n",
    "\n",
    "\\begin{equation}\n",
    "w_{grades} \\cdot x_{grades} + w_{test} \\cdot x_{test} = -1 \\cdot x_{grades} - 0.2 \\cdot x_{test}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The perceptron applies these weights to the inputs and sums them in a __linear combination__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The next section is verbose as it tries to skirt around the math notation - it's just summations\n",
    "- Of `x_input` and `w_input`, which is the __input__ and the __weight__ associated with said __input__ \n",
    "  - `x` differentiates it, nothing more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Summatively, that looks like:\n",
    "\n",
    "\\begin{equation}\n",
    "\\sum_{i=1}^m w_{i} \\cdot x_{i}\n",
    "\\end{equation}\n",
    "\n",
    "- Where *m* is the number of inputs\n",
    "  - This can be omitted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Please Note\n",
    "that the relative size of weights is what's most important, not the weights themselves\n",
    "\n",
    "- He uses an example of\n",
    "  - w_grades = -1\n",
    "  - w_test = -.2\n",
    "- And points out that the ratio of 5:1 is what's important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__In brief__, multiply an input by its weight\n",
    "\n",
    "- Take an opinion by its importance factor\n",
    "- It's that easy.\n",
    "  - Now to learn about these activation functions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Output: an Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First the analogy that makes the most since in my mind:\n",
    "> Consider an actual neuron in the brain:\n",
    "- It absorbs charge from its neighbors\n",
    "- After a certain amount of input, it discharges\n",
    "\n",
    "> When the activation function fires, the neuron discharges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__He shows__ an activation function as mearly a conditional function:\n",
    "\n",
    "\\begin{equation}\n",
    "f(h) = \\{_{1 if h \\geq 0}^{0 if h \\lt 0}\n",
    "\\end{equation}\n",
    "\n",
    "- Apparently, this function is called the __Heaviside Step Function__\n",
    "  - This sucks for university acceptance :P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "This guy is really making this seem too easy.\n",
    "- I get that the math is simple, but his round-about way of explaining it is frustrating\n",
    "  - I know how to graph on a chart!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we've written a dodgy function (only poor students activate). We'll fix it\n",
    "- The instructor's finally getting to the point!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias, or \"shifting the goods\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have a good shape (to the graph of acceptable inputs), but we don't like the selection.\n",
    "- Just add __bias__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Vocabulary:\n",
    "\n",
    "  - __bias__ is a translation of the graph of acceptable inputs.\n",
    "    - Simple *adding* to the result of the base equation\n",
    "    - First demonstrated on the y-axis.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "For feedback:\n",
    "\n",
    "> This feels like a bad way to explain this. Sure, not everyone remembers mathematics as I do. But this doesn't do the equations justice, and I think it would be inappropriate for students to feel confident with these hand-wave-y definitions\n",
    "\n",
    "- The math is clear, but the words are __not__.\n",
    "  - Almost has me doubting my math\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More to my point, I'm just going to copy the instructors paragraph..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Note to self:\n",
    "\n",
    "- I keep slipping in and out of the Feymann techinque.\n",
    "- I want to rearticulate it, but there's nothing to say\n",
    "  - __ADD__ a bias. Literally.\n",
    "    - Some number, by the science of neural networks, will be generated to shift the good shape to have the right coverage\n",
    "  - I'm not going to re-explain [what summation is](https://en.wikipedia.org/wiki/Summation)\n",
    "    - I've been familiar with this concept for longer than I've been programming.\n",
    "    - Here's my attempt at it:\n",
    "      - For every *single* number between \\*points to start and end of the summation\\*\n",
    "      - Solve the equation with __that__ number as a result\n",
    "      - Add __all__ of those results together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![perceptron formula](https://d17h27t6h515a5.cloudfront.net/topher/2017/February/58951180_perceptron-equation-2/perceptron-equation-2.gif \"Perceptron Formula\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the aformentioned, poor, presumptive summary:\n",
    "\n",
    "> This formula returns $1$ if the input ($x_{1}$,$x_{2}$,...,$x_{m}$) belongs to the accepted-to-university category or returns 0 if it doesn't. The input is made up of one or more real numbers, each one represented by $x_i$, where $m$ is the number of inputs.\n",
    "\n",
    "Then the neural network starts to learn! Initially, the weights ( $w_{i}$ ) and bias ( $b$ ) are assigned a random value, and then they are updated using a learning algorithm like gradient descent. The weights and biases change so that the next training example is more accurately categorized, and patterns in data are \"learned\" by the neural network.\n",
    "\n",
    "Now that you have a good understanding of perceptions, let's put that knowledge to use. In the next section, you'll create the AND perceptron from the Neural Networks video by setting the values for weights and bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While I feel like I'm growing comfortable with Neural Networks, I'm seriously disappointed in this section\n",
    "- \"Uhhh... We can't figure out how to teach this to you, so we're going to give you an impartial __text__ lecture of that is heavy on mathematics, but without the explanations of their necessity\"\n",
    "- It's like Siraj's videos, but less goofy; more stoic\n",
    "  - That is __not__ a good thing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quizzes, or \"suffering will be your teacher\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "This first quiz feels almost unfair, given the abyssmal wrap up of the last section.\n",
    "\n",
    "- I'm about to read another resource to get together a more decent understanding of the activation.\n",
    "  - I can appreciate the abiguity of \"you define it however you want\", but what it this threshold for effective\n",
    "    - How could you ever know before hand?\n",
    "  - I don't imagine the \"Heaviside step function\" will always suffice for an activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a \"fill in the blank\", with the weights of the inputs, a bunch of graphs showing desired behavior, and frustration...\n",
    "- They want the AND function first..\n",
    "  - It's contrived instruction, to get us used to using the $b$\n",
    "- `AND` can be programmed much more simply."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "So, some how, changing the weights and bias for these logic gate neural networks is relaxing my mind.\n",
    "\n",
    "- My brain is just treating them like puzzles.\n",
    "  - I still don't know if this is a good thing\n",
    "- After I gave the answer *it wanted* for \"how would we shift this graph to go from AND to OR?\"\n",
    "  - For the record: \"Increase the weights\" and \"Decrease the magnitude of the bias\"\n",
    "\n",
    "__ALL__ of these are still using the aforementioned biased step function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "Back at it:\n",
    "> A neural network is like any tool. You have to know when to use it.\n",
    "\n",
    "Thankful they didn't make me play the \"weight puzzle game\" with XOR - it's always a pain.\n",
    "- Thankfully, the XOR is a composition of\n",
    "  - two NOT gates\n",
    "  - two AND gates\n",
    "  - one OR gate\n",
    "  - a few passthroughs to give the illusion of \"layers\".\n",
    "- It's __very__ similar to the circuit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The power of a neural network isn't building it by hand, like we were doing. It's the ability to learn from examples. In the next few sections, you'll learn how a neural networks sets it's own weights and biases.\n",
    "\n",
    "... Yeah. Hopefully.\n",
    "- I feel like I've learned a lot of vocabulary, and a few theoretical applications of math I've known for ages...\n",
    "  - And refreshed my Linear Algebra, which is always good.\n",
    "- I've seen glimpses of the applications of that Matrix Math in these neural systems\n",
    "  - I want to weild that power...\n",
    "  - There, I said it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mat returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "He gives a breakdown of all that goes into a neuron:\n",
    "- Inputs\n",
    "  - And associated weights\n",
    "- A bias\n",
    "- resultant input, $h$\n",
    "- Activation function $f(h)$\n",
    "- Output, $y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drawn as such:\n",
    "\n",
    "![image](https://d17h27t6h515a5.cloudfront.net/topher/2017/February/589366f0_simple-neuron/simple-neuron.png \"Cirles are units, boxes are operations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is rather decent, leading to some insights:\n",
    "\n",
    "- Construction of the activation function argument\n",
    "  - Weighted input, with bias\n",
    "  - In other words: $x_1 \\cdot w_1 + b$\n",
    "- The argument to the activation function: $h$\n",
    "- The exit - and output - is __activation function__: $y = f(h)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "My mind understands composition.\n",
    "\n",
    "This a layer, a composition, of trivial operations\n",
    "\n",
    "- Multiplication\n",
    "- Addition\n",
    "- Identity\n",
    "\n",
    "... Along with some arbitrary activation function.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mat has us program the [Sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function) and they keep repeating this:\n",
    "\n",
    "> stacking units will let you model linearly inseparable data, impossible to do with regression models.\n",
    "\n",
    "This phrase \"linearly inseperable data\" has been said four times now.\n",
    "- I haven't heard it before\n",
    "- But I don't think many operations a above decomposition.\n",
    "  - I actually think there are infinitely many operations which can be decomposed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "I'm not going to run the code, but here it is:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    # TODO: Implement sigmoid function\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "inputs = np.array([0.7, -0.3])\n",
    "weights = np.array([0.1, 0.8])\n",
    "bias = -0.1\n",
    "\n",
    "# TODO: Calculate the output\n",
    "output = sigmoid(np.dot(weights, inputs) + bias)\n",
    "\n",
    "print('Output:', output)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "## Learning Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we don't know the correct weights, we have to learn them from history.\n",
    "\n",
    "- Example data\n",
    "\n",
    "Then we can then predict (produce outputs) with those weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Know when you're wrong\n",
    "\n",
    "Since we start with random weights, we need to adjust those weights towards what is proper\n",
    "\n",
    "- *iterate*, if you will"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__So we *measure*__ how wrong we are, to gather some sense of how to course correct\n",
    "\n",
    "- i.e. \"Too big\", \"too big\", \"too hot\", \"too cold\", \"just right\"\n",
    "  - Thanks little Red\n",
    "\n",
    "One of the best methods for doing this is the \"[Sum of Squared Errors](https://en.wikipedia.org/wiki/Residual_sum_of_squares)\":\n",
    "\n",
    "\\begin{equation}\n",
    "E = \\frac 1 2 \\sum{_\\mu} \\sum{_j} [y_j^\\mu - \\hat{y}_j^\\mu]^2\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's break it down\n",
    "\n",
    "Pseudo-academia (me):\n",
    "> Take half of the sum of the sum of:\n",
    "  - all outputs (predictions) subtracted from the true value for some given data point $\\mu$ and a predction $j$ squared\n",
    "\n",
    "Layman's terms:\n",
    "> For every data point (in our training set) calculate the inner sum of \n",
    "- The squared difference of\n",
    "- The true value (categorization [think back]) of a data point, and the prediction for that data point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "We're building a graph and we need to get the shape right.\n",
    "- The squaring is probably inherited from statistical models that fit to a curve. \n",
    "  - Or correlation against a best fit line\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "오후 11시 9분 끝났습니다...\n",
    "\n",
    "Having studied hard, had an awful work day, and endured ill-suited-for-me instruction (\"intellectual babble\", that uses all the right words sans substance), I just want to go to sleep.\n",
    "- My chest is heavy.\n",
    "  - It hurts to feel this much angst\n",
    "  - It's disappointing that getting started is such a labor\n",
    "- Was this really worth \\$600?\n",
    "  - When I've got so many things I could have read for free\\?\n",
    "  - When the, arguably, most important lesson in this course isn't singing true like the previous math lesson\\?\n",
    "    - 왜? 어떡해? :("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
