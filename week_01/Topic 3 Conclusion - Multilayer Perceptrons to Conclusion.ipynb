{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "When are they going to call them Sigmoid neurons, if ever?\n",
    "\n",
    "- I need a new definition of perceptron, if they aren't constricted to binary input and output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Before, we were dealing with only one output node which made the code straightforward. However now that we have multiple input units and multiple hidden units, the weights between them will require two indices: $w_{ij}$ where $i$ denotes input units and $j$ are the hidden units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The indices on $w$ are like matrix indices. Nothing more complicated\n",
    "  - Just where the dimensions are coming from changed :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine the network:\n",
    "\n",
    "![image](https://d17h27t6h515a5.cloudfront.net/topher/2017/February/589978f4_network-with-labeled-weights/network-with-labeled-weights.png \"Weights are labeled with the __input__ source node and the __hidden layer__ destination node\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's funny that this notation is like a matrix, because __we store these in matrices__\n",
    "\n",
    "- That's right, our weights array just became a matrix (at least)\n",
    "  - Tensors will flow, in due time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also note:\n",
    "\n",
    "- Rows will be all weights leading __out__ of a __single Input Node__\n",
    "  - Depicted by the first index of the matrix\n",
    "- In the following image, note that everything in a given column will be taken in for given\n",
    "  - This is the second index of the matrix\n",
    "\n",
    "![weighted labeled hidden layer](https://d17h27t6h515a5.cloudfront.net/topher/2017/February/58a49908_multilayer-diagram-weights/multilayer-diagram-weights.png \"Relax and let the data flow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining the following code, a few things become apparent:\n",
    "\n",
    "```python\n",
    "# Number of records and input units\n",
    "n_records, n_inputs = features.shape\n",
    "# Number of hidden units\n",
    "n_hidden = 2\n",
    "weights_input_to_hidden = np.random.normal(0, n_inputs**-0.5, size=(n_inputs, n_hidden))\n",
    "```\n",
    "\n",
    "1. $n^{\\frac {-1} 2} = 1/{n^{\\frac 1 2}}  = 1/{\\sqrt{n}}$\n",
    "  - We used the latter in the previous sections.\n",
    "  - We use the former now. More concise\n",
    "2. `size=...` is the dimensionality that comes with this being a __matrix__ now\n",
    "3. In the example, the number of hidden nodes is trival and appears irrelevant\n",
    "  - But we know that the layers provide deeper insight...\n",
    "  - There may be more to this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining the node-input is easy\n",
    "\n",
    "... right after you get a dot product\n",
    "\n",
    "Let's use $h_1$ for this\n",
    "\n",
    "- Its weighted inputs are the dot product of the inputs - $x_1, x_2, x_3$ - and the hidden layer weights for just its column - $w_{11}, w_{21}, w_{31}$\n",
    "  - See the orange above\n",
    "  - Important: \"__The inputs__\" to $h_1$ are a vector\n",
    "    - As is its column, taken independently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we get something a little like:\n",
    "\n",
    "![h1 weighted inputs](https://d17h27t6h515a5.cloudfront.net/topher/2017/January/588ae392_codecogseqn-2/codecogseqn-2.png \"Notice this is just h1...\")\n",
    "\n",
    "- And we'd have to do the same thing for $h_2$...\n",
    "\n",
    "\\begin{equation}\n",
    "h_2 = x_1 w_{12} + x_2 w_{22} + x_3 w_{32}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- But wouldn't that get a little long winded?\n",
    "  - Yes, little Timmy. I believe it would...\n",
    "\n",
    "So hows about we just take the cross product (vector x matrix) and get back a vector of the hidden, weighted input values?\n",
    "\n",
    "- Seems legit. Simple, straight forward\n",
    "- Do the math in one swing :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks a little something like:\n",
    "\n",
    "\\begin{equation*}\n",
    "h_j = x \\times w = \\begin{vmatrix}\n",
    "x_1 & x_2 & x_3\n",
    "\\end{vmatrix} \\times \\begin{vmatrix}\n",
    "w_{11} & w_{12} \\\\\n",
    "w_{21} & w_{22} \\\\\n",
    "w_{31} & w_{32}\n",
    "\\end{vmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "And that outputs a vector of:\n",
    "\n",
    "\\begin{vmatrix}\n",
    "{x \\cdot w_{i1}} & {x \\cdot w_{i2} }\n",
    "\\end{vmatrix}\n",
    "\n",
    "- Where we let $i$ stand in for the row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So just like we talked about earlier :) Instead of column-wise, one-at-a-time, we just do the matrix maths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A word of caution to this tale\n",
    "\n",
    "You could very well setup the inputs as a column, and transpose the matrix.\n",
    "\n",
    "- Just be aware that the rows would become the hiddens' inputs, and the columns would be a given nodes' outputs\n",
    "\n",
    "That would look something like:\n",
    "\n",
    "![column-flipped hidden layer](https://d17h27t6h515a5.cloudfront.net/topher/2017/January/588b7c74_inputs-matrix/inputs-matrix.png \"Oh the choices available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "And for the sake of being unabashedly clear:\n",
    "\n",
    "Where a vector is an array, in code...\n",
    "\n",
    "- If you want to multiply __matrix-by-vector__\n",
    "  - the numer of columns in the matrix must be the rows of the vector\n",
    "    - Like in the above picture\n",
    "- \" \" __vector-by-matrix__\n",
    "  - Vector's length must match the number of columns of the matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "And just some code, for icing on the cake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nuances of NumPy vectors\n",
    "\n",
    "> You see above that sometimes you'll want a column vector, even though by default Numpy arrays work like row vectors. It's possible to get the transpose of an array like so `arr.T`, but for a 1D array, the transpose will return a row vector. Instead, use `arr[:,None]` to create a column vector.\n",
    "\n",
    "That said, here's what it looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = np.random.normal(size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.14418608 -0.57416225  0.66976266]\n"
     ]
    }
   ],
   "source": [
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.14418608 -0.57416225  0.66976266]\n"
     ]
    }
   ],
   "source": [
    "print(features.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.14418608]\n",
      " [-0.57416225]\n",
      " [ 0.66976266]]\n"
     ]
    }
   ],
   "source": [
    "print(features[:, None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or you could just tell NumPy to give you a 2-D vector, so you can work with it in a matrix-transpository way :P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.14418608, -0.57416225,  0.66976266]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(features, ndmin=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.14418608],\n",
       "       [-0.57416225],\n",
       "       [ 0.66976266]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(features, ndmin=2).T # there's that column we love"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a coding test, to implement a 4x3x2 feed-forward network\n",
    "\n",
    "- We're calling them \"hidden\" layers, because soon they were be generated programmatically.\n",
    "\n",
    "The exercise:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# Network size\n",
    "N_input = 4\n",
    "N_hidden = 3\n",
    "N_output = 2\n",
    "\n",
    "np.random.seed(42)\n",
    "# Make some fake data\n",
    "X = np.random.randn(4)\n",
    "\n",
    "weights_input_to_hidden = np.random.normal(0, scale=0.1, size=(N_input, N_hidden))\n",
    "weights_hidden_to_output = np.random.normal(0, scale=0.1, size=(N_hidden, N_output))\n",
    "\n",
    "\n",
    "# TODO: Make a forward pass through the network\n",
    "\n",
    "hidden_layer_in = None\n",
    "hidden_layer_out = None\n",
    "\n",
    "print('Hidden-layer Output:')\n",
    "print(hidden_layer_out)\n",
    "\n",
    "output_layer_in = None\n",
    "output_layer_out = None\n",
    "\n",
    "print('Output-layer Output:')\n",
    "print(output_layer_out)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I'm a little caught on how to produce the hidden layer's output\n",
    "  - I would think it's just a mapping of the sigmoid function over the arrays\n",
    "  - But that feels weird to me"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz reflection\n",
    "\n",
    "Turns out that \"mapping\" the function was the right approach, because that's exactly what happens.\n",
    "\n",
    "- This is most optimally (performance) done by [`np.vectorize`](https://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.vectorize.html)\n",
    "\n",
    "```python\n",
    "# Given that everything is activated by a sigmoid function\n",
    "activation_func = np.vectorize(sigmoid)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point the only thing one would question is probably:\n",
    "\n",
    "- What was the rest of the solution?\n",
    "\n",
    "__I finally understand the simplicity__\n",
    "\n",
    "- Just invoke the function (Functional Programming is a straight flush here) for all relevant nodes\n",
    "  - The activation function __is__ the function for the output\n",
    "  - If it's a Sigmoid, then pass $h$ as a parameter to that function\n",
    "  - Boom! There's your output\n",
    "\n",
    "```python\n",
    "# input layer\n",
    "X = np.random.randn(4)\n",
    "\n",
    "hidden_layer_inputs = np.dot(X, weights_input_to_hidden)\n",
    "# here's that simplicity\n",
    "hidden_layer_output = activation_func(hidden_layer_inputs)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The mathematics here is \n",
    "\n",
    "- Recursive in logic\n",
    "  - Do this algorithm throughout the network, until there are no more \"child\" nodes\n",
    "- Iterative in explanation\n",
    "  - For every column, do this operation *omitted*\n",
    "  - Pass those results to the next column\n",
    "  - Repeat\n",
    "\n",
    "Where are the code is... iterative.\n",
    "\n",
    "Just food for thought"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "I imagine we're not very far at all from just inlining the input calculation.\n",
    "\n",
    "- But wait. That doesn't make any sense.\n",
    "  - Need the inputs available to calculate the errors...\n",
    "\n",
    "Shoot. Almost.\n",
    "\n",
    "- Though I'm not really a fan of inlining :P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Turns out you can just call a function on an `np.array` and it will behave as you expect it to.\n",
    "\n",
    "- Didn't, necessarily, need to \"map\" via `vectorize`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
