{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "When are they going to call them Sigmoid neurons, if ever?\n",
    "\n",
    "- I need a new definition of perceptron, if they aren't constricted to binary input and output.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Before, we were dealing with only one output node which made the code straightforward. However now that we have multiple input units and multiple hidden units, the weights between them will require two indices: $w_{ij}$ where $i$ denotes input units and $j$ are the hidden units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The indices on $w$ are like matrix indices. Nothing more complicated\n",
    "  - Just where the dimensions are coming from changed :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine the network:\n",
    "\n",
    "![image](https://d17h27t6h515a5.cloudfront.net/topher/2017/February/589978f4_network-with-labeled-weights/network-with-labeled-weights.png \"Weights are labeled with the __input__ source node and the __hidden layer__ destination node\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's funny that this notation is like a matrix, because __we store these in matrices__\n",
    "\n",
    "- That's right, our weights array just became a matrix (at least)\n",
    "  - Tensors will flow, in due time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also note:\n",
    "\n",
    "- Rows will be all weights leading __out__ of a __single Input Node__\n",
    "  - Depicted by the first index of the matrix\n",
    "- In the following image, note that everything in a given column will be taken in for a given node\n",
    "  - This is the second index of the matrix\n",
    "\n",
    "![weighted labeled hidden layer](https://d17h27t6h515a5.cloudfront.net/topher/2017/February/58a49908_multilayer-diagram-weights/multilayer-diagram-weights.png \"Relax and let the data flow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining the following code, a few things become apparent:\n",
    "\n",
    "```python\n",
    "# Number of records and input units\n",
    "n_records, n_inputs = features.shape\n",
    "# Number of hidden units\n",
    "n_hidden = 2\n",
    "weights_input_to_hidden = np.random.normal(0, n_inputs**-0.5, size=(n_inputs, n_hidden))\n",
    "```\n",
    "\n",
    "1. $n^{\\frac {-1} 2} = 1/{n^{\\frac 1 2}}  = 1/{\\sqrt{n}}$\n",
    "  - We used the latter in the previous sections.\n",
    "  - We use the former now. More concise\n",
    "2. `size=...` is the dimensionality that comes with this being a __matrix__ now\n",
    "  - Assignment via a tuple is a rather elegant way to go about it\n",
    "3. In the example, the number of hidden nodes is trival and appears irrelevant\n",
    "  - But we know that the layers provide deeper insight...\n",
    "    - Specifically because networks can grow the number of nodes as they learn the problem\n",
    "    - Meta-learning, in a way\n",
    "  - There may be more to this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining the node-input is easy\n",
    "\n",
    "... right after you get a dot product\n",
    "\n",
    "Let's use $h_1$ for this\n",
    "\n",
    "- Its weighted inputs are the dot product of the inputs - $x_1, x_2, x_3$ - and the hidden layer weights for just its column - $w_{11}, w_{21}, w_{31}$\n",
    "  - See the orange above\n",
    "  - Important: \"__The inputs__\" to $h_1$ are a vector\n",
    "    - As is its column, taken independently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we get something a little like:\n",
    "\n",
    "![h1 weighted inputs](https://d17h27t6h515a5.cloudfront.net/topher/2017/January/588ae392_codecogseqn-2/codecogseqn-2.png \"Notice this is just h1...\")\n",
    "\n",
    "- And we'd have to do the same thing for $h_2$...\n",
    "\n",
    "\\begin{equation}\n",
    "h_2 = x_1 w_{12} + x_2 w_{22} + x_3 w_{32}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- But wouldn't that get a little long winded?\n",
    "  - Yes, little Timmy. I believe it would...\n",
    "\n",
    "So hows about we just take the cross product (vector x matrix) and get back a vector of the hidden, weighted input values?\n",
    "\n",
    "- Seems legit. Simple, straight forward\n",
    "- Do the math in one swing :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks a little something like:\n",
    "\n",
    "\\begin{equation*}\n",
    "h_j = x \\times w = \\begin{vmatrix}\n",
    "x_1 & x_2 & x_3\n",
    "\\end{vmatrix} \\times \\begin{vmatrix}\n",
    "w_{11} & w_{12} \\\\\n",
    "w_{21} & w_{22} \\\\\n",
    "w_{31} & w_{32}\n",
    "\\end{vmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "And that outputs a vector of:\n",
    "\n",
    "\\begin{vmatrix}\n",
    "{x \\cdot w_{i1}} & {x \\cdot w_{i2} }\n",
    "\\end{vmatrix}\n",
    "\n",
    "- Where we let $i$ stand in for the row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So just like we talked about earlier :) Instead of column-wise, one-at-a-time, we just do the matrix maths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A word of caution to this tale\n",
    "\n",
    "You could very well setup the inputs as a column, and transpose the matrix.\n",
    "\n",
    "- Just be aware that the rows would become the hiddens' inputs, and the columns would be a given nodes' outputs\n",
    "\n",
    "That would look something like:\n",
    "\n",
    "![column-flipped hidden layer](https://d17h27t6h515a5.cloudfront.net/topher/2017/January/588b7c74_inputs-matrix/inputs-matrix.png \"Oh the choices available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "And for the sake of being unabashedly clear:\n",
    "\n",
    "Where a vector is an array, in code...\n",
    "\n",
    "- If you want to multiply __matrix-by-vector__\n",
    "  - the numer of columns in the matrix must be the rows of the vector\n",
    "    - Like in the above picture\n",
    "- \" \" __vector-by-matrix__\n",
    "  - Vector's length must match the number of columns of the matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "It's really quite humorous that this is a point they keep harping on\n",
    "\n",
    "> Don't mix up your dimensions, or else your code won't compile.\n",
    "\n",
    "Yup... Makes sense.\n",
    "\n",
    "- Maybe I've just seen so much bad Java, that has 10+ parameters to a function, that I just \"get it\"\n",
    "- Or maybe it's the fact that I still have the algebra *relatively* fresh in mind\n",
    "  - Getting fresher all the time, reviewing all of grade school with [Khan academy](https://www.khanacademy.org/mission/pre-algebra)\n",
    "\n",
    "---\n",
    "And just some code, for icing on the cake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nuances of NumPy vectors\n",
    "\n",
    "> You see above that sometimes you'll want a column vector, even though by default Numpy arrays work like row vectors. It's possible to get the transpose of an array like so `arr.T`, but for a 1D array, the transpose will return a row vector. Instead, use `arr[:,None]` to create a column vector.\n",
    "\n",
    "That said, here's what it looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = np.random.normal(size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.47491054  0.43920491 -1.5094122 ]\n"
     ]
    }
   ],
   "source": [
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.47491054  0.43920491 -1.5094122 ]\n"
     ]
    }
   ],
   "source": [
    "print(features.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.47491054]\n",
      " [ 0.43920491]\n",
      " [-1.5094122 ]]\n"
     ]
    }
   ],
   "source": [
    "print(features[:, None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or you could just tell NumPy to give you a 2-D vector, so you can work with it in a matrix-transpository way :P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.47491054,  0.43920491, -1.5094122 ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(features, ndmin=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.47491054],\n",
       "       [ 0.43920491],\n",
       "       [-1.5094122 ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(features, ndmin=2).T # there's that column we love"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a coding test, to implement a 4x3x2 feed-forward network\n",
    "\n",
    "- We're calling them \"hidden\" layers, because soon they were be generated programmatically.\n",
    "  - At least, that is my belief\n",
    "\n",
    "The exercise skeleton code:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# Network size\n",
    "N_input = 4\n",
    "N_hidden = 3\n",
    "N_output = 2\n",
    "\n",
    "np.random.seed(42)\n",
    "# Make some fake data\n",
    "X = np.random.randn(4)\n",
    "\n",
    "weights_input_to_hidden = np.random.normal(0, scale=0.1, size=(N_input, N_hidden))\n",
    "weights_hidden_to_output = np.random.normal(0, scale=0.1, size=(N_hidden, N_output))\n",
    "\n",
    "\n",
    "# TODO: Make a forward pass through the network\n",
    "\n",
    "hidden_layer_in = None\n",
    "hidden_layer_out = None\n",
    "\n",
    "print('Hidden-layer Output:')\n",
    "print(hidden_layer_out)\n",
    "\n",
    "output_layer_in = None\n",
    "output_layer_out = None\n",
    "\n",
    "print('Output-layer Output:')\n",
    "print(output_layer_out)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I'm a little caught on how to produce the hidden layer's output\n",
    "  - I would think it's just a mapping of the sigmoid function over the arrays\n",
    "  - But that feels weird to me"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz reflection\n",
    "\n",
    "Turns out that \"mapping\" the function was the right approach, because that's exactly what happens.\n",
    "\n",
    "- This is most optimally (performance) done by [`np.vectorize`](https://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.vectorize.html)\n",
    "\n",
    "```python\n",
    "# Given that everything is activated by a sigmoid function\n",
    "activation_func = np.vectorize(sigmoid)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point the only thing one would question is probably:\n",
    "\n",
    "- What was the rest of the solution?\n",
    "\n",
    "__I finally understand the simplicity__\n",
    "\n",
    "- Just invoke the function (Functional Programming is a straight flush here) for all relevant nodes\n",
    "  - The activation function __is__ the function for the output\n",
    "  - If it's a Sigmoid, then pass $h$ as a parameter to that function\n",
    "  - Boom! There's your output\n",
    "\n",
    "```python\n",
    "# input layer\n",
    "X = np.random.randn(4)\n",
    "\n",
    "hidden_layer_inputs = np.dot(X, weights_input_to_hidden)\n",
    "# here's that simplicity\n",
    "hidden_layer_output = activation_func(hidden_layer_inputs)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The mathematics here is \n",
    "\n",
    "- Recursive in logic\n",
    "  - Do this algorithm throughout the network, until there are no more \"child\" nodes\n",
    "- Iterative in explanation\n",
    "  - For every column, do this operation *omitted*\n",
    "  - Pass those results to the next column\n",
    "  - Repeat\n",
    "\n",
    "Where are the code is... iterative.\n",
    "\n",
    "Just food for thought"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "I imagine we're not very far at all from just inlining the input calculation.\n",
    "\n",
    "- But wait. That doesn't make any sense.\n",
    "  - Need the inputs available to calculate the errors...\n",
    "\n",
    "Shoot. Almost.\n",
    "\n",
    "- Though I'm not really a fan of inlining :P\n",
    "\n",
    "---\n",
    "In review (3날후) I say this bit about inlining because each node is a little calculator:\n",
    "\n",
    "- You clean up what goes in (input $h$)\n",
    "- Then it just spits out the output of some simple function\n",
    "  - Often the result of a Sigmoid.\n",
    "\n",
    "And __that's it__. Thus, \"nearly-inline-able\" :P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Turns out you can just call a function on an `np.array` and it will behave as you expect it to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3 4]\n"
     ]
    }
   ],
   "source": [
    "map_example_arr = np.array([2,3,4])\n",
    "print(map_example_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10, 15, 20])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_example_arr * 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Didn't, necessarily, need to \"map\" via `vectorize`\n",
    "  - Might in the future, with larger tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We're dealing with multiple layers, but we'd still like to train with Gradient Descent.\n",
    "\n",
    "We've already learned that error in the output node is $\\delta = (y - \\hat{y})f'(h)$\n",
    "\n",
    "- But this was only with one layer\n",
    "- __In our case__, this produces the error between the hidden layer and output layer\n",
    "\n",
    "So... How do we get the error between the input layer and the hidden layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we find the error to use in the gradient descent step?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Error for units is proportional to the error in the output layer *times* the weight between the units.\n",
    "\n",
    "Remember that analogy, that this is taking an opinion from a person?\n",
    "\n",
    "- Now imagine that *they* heard it through the grapevine.\n",
    "- Not very credible now, are they?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__In other words__\n",
    "\n",
    "The unit with a __strong weight__ to the output will contribute a __stronger error__\n",
    "\n",
    "- Before training of course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following picture, not the subscripts on our error-term $\\delta$ and weights $W_1$ and $W_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Are those weights for a column? (matrix column; for more, see above \"column\" usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "By the way, that $\\delta$ is lowercase Greek __delta__. Haven't mentioned it all this time.\n",
    "\n",
    "Sorry!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![weighted backprop level 1](./weighted backpropogation through layers.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Instead of __propogating__ your inputs *forward*, you're __propogating__ the error *__backwards__*\n",
    "\n",
    "- This is where this exercise gets its name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also imagine that you are feeding your error, as input, into a mirror network\n",
    "\n",
    "- Everything is laid out exactly reflected as to how they were originally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropogation Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "I would say that mathematics is *succint*, once you understand what the symbols are there for.\n",
    "\n",
    "- Otherwise, it will just appear to be a foreign language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Calculating the error for the output layers is a on a per-output-node basis.\n",
    "\n",
    "Observe the following equation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\delta_j^h = \\sum W_{jk} \\delta_k^o f'(h_j)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notation breakdown:\n",
    "\n",
    "- $x^o$ is identifying our __o__utput-related nodes\n",
    "- $k$ signifies how many output nodes there are for the current layers in the back propogation\n",
    "- $\\delta_j^h$ is the error-term of some $h$ in the hidden-layer\n",
    "- $x_j$ identifies which node we're talking about in the given hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabulary:\n",
    "\n",
    "- __current__ is meant to simplify the observation of the problem\n",
    "  - If you are *recursively iterating* back through each layer, to correct the error, you are aware of two contexts\n",
    "    - The layer you previously oserved\n",
    "      - On __first iteration__ this is the output layer\n",
    "    - The layer currently being observed\n",
    "      - (In a 3x2x1 network) On __first iteration__, this is the 2-node layer\n",
    "  - To take a programmatic lens to it\n",
    "    - In functional programming, a `reduce : [T] -> T -> a -> a` can be written (logically) as either perspective\n",
    "      - \"Current, next\":\n",
    "        - I've got the new data I'm building and the next item to consume\n",
    "      - \"Current, previous\":\n",
    "        - I've got the thing that is being built and the current item to consume\n",
    "  - Then try looking at backpropogation in that way\n",
    "    - This is a stretch for holistically absorbing this concept\n",
    "    - But everything in programming can be written as a reduce ;) (or map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the new equation for calculating the *next step of our gradient descent*:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![new-old weight_ij](https://d17h27t6h515a5.cloudfront.net/topher/2017/January/588bc2d4_backprop-general/backprop-general.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__It's the same as the old one!__ Great!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to take steps and eventually reach our goal, like walking isn't a novel experience for me\n",
    "> That's expressed by our friend $V_{in}$.\n",
    "- He's the value that got us here (be it on- or off-course)\n",
    "\n",
    "I want to walk in the right direction, eventually reaching the bottom of the valley\n",
    "> That direction is easily discernable from our friend $\\delta$\n",
    "\n",
    "If I'm going in the correct direction, I want to run - not walk.\n",
    "> That's what $\\eta$ is for. Bigger strides, not just \"steps\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Mat takes a moment here to walk through an example. I will notate anything new that pops out.\n",
    "\n",
    "> Again, __note__ this is \"Lesson 8: Intro to Neural Networks, 15. Backpropogation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes from \"working through an example\"\n",
    "\n",
    "- $a$ for __a__ctivation, is the definition of the result of the output function\n",
    "  - \"One *outputs* an *activation*\"\n",
    "- $o^h$ should be read as \"the *output* for a given $h$\"\n",
    "  - Or a particular weigthed input, if that suits your fancy.\n",
    "- $\\eta$ has utility\n",
    "  - Because these numbers are so small, but there's so much to correct for, it's good to scale them up\n",
    "    - In the example the $\\delta_$ (error-term of the output) is about half of the target\n",
    "    - But the $\\delta^h$ is about 0.003.\n",
    "    - Needless to say, that would take a long time to correct (especially given how low the weights are genarl)\n",
    "\n",
    "---\n",
    "What I just observed is called the __vanishing gradient problem__.\n",
    "\n",
    "> Because the maximum derivative of a Sigmoid function 0.25, the errors in the output layer get reduced by at least 75%\n",
    "- And the hidden layer before it 93.75%!\n",
    "  - Check the math (the first one is just subtraction. The next is harder)\n",
    "\n",
    "Using a sigmoid activation function quickly reduces the weights steps (which we *try* to counteract with the learning rate)\n",
    "\n",
    "- __Why not just use a better activation function?__\n",
    "\n",
    "Speculation:\n",
    "\n",
    "- This might just be proof that certain activations model the \"butterfly effect\" better than others.\n",
    "  - That's hard to say, given how new I am to this\n",
    "  - We need young minds to ask the basic, out-of-the-box questions\n",
    "  - We need learned minds to reconsider what they feel is impossible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing in Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to consider the error for *each unit*\\* in the hidden layer\n",
    "\n",
    "- I've been correcting it, but for *practically* this whole entry, Mat (or whoever) has been using *unit* in place of *node/neuron/perceptron*.\n",
    "  - For the love of Pete, please! Some consistency would be nice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Row x row multiplication sucks, so do the transposition trick we learned earlier:\n",
    "\n",
    "```python\n",
    "some_inputs[:, None] # BAM! Got a column now\n",
    "```\n",
    "\n",
    "- Matrix math, of course, has to have the proper dimensions\n",
    "  - So check yourself at the door\n",
    "- The instructor says we're likely to have a different number of *hidden* vs *input* units, so just straight multiplication across will fail\n",
    "  - Again, 당연아지. The dimensions have to match and (x,) $\\times$ (y,) doesn't exactly work\n",
    "  - The math falls through"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming exercise\n",
    "To see how we're holding up\n",
    "\n",
    "As before, the skeleton code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "x = np.array([0.5, 0.1, -0.2])\n",
    "target = 0.6\n",
    "learnrate = 0.5\n",
    "\n",
    "weights_input_hidden = np.array([[0.5, -0.6],\n",
    "                                 [0.1, -0.2],\n",
    "                                 [0.1, 0.7]])\n",
    "\n",
    "weights_hidden_output = np.array([0.1, -0.3])\n",
    "\n",
    "## Forward pass\n",
    "hidden_layer_input = np.dot(x, weights_input_hidden)\n",
    "hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "\n",
    "output_layer_in = np.dot(hidden_layer_output, weights_hidden_output)\n",
    "output = sigmoid(output_layer_in)\n",
    "\n",
    "## Backwards pass\n",
    "## TODO: Calculate output error\n",
    "error = target - output\n",
    "\n",
    "# TODO: Calculate error term for output layer\n",
    "# inline f'(h) of sigmoid (output * (1 - output))\n",
    "output_error_term = error * output * (1 - output)\n",
    "\n",
    "# TODO: Calculate error term for hidden layer\n",
    "# f'(h) = hidden_layer_output\n",
    "# o^o = output_error_term\n",
    "# W = weight between current and previous layer\n",
    "hidden_error_term = weights_hidden_output * output_error_term * hidden_layer_output\n",
    "\n",
    "# TODO: Calculate change in weights for hidden layer to output layer\n",
    "delta_w_h_o = learnrate * output_error_term * hidden_layer_output\n",
    "\n",
    "# TODO: Calculate change in weights for input layer to hidden layer\n",
    "delta_w_i_h = learnrate * hidden_error_term * x\n",
    "\n",
    "print('Change in weights for hidden layer to output layer:')\n",
    "print(delta_w_h_o)\n",
    "print('Change in weights for input layer to hidden layer:')\n",
    "print(delta_w_i_h)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And my solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was really as simple as following the equations\n",
    "\n",
    "- I see why they use the variable names\n",
    "  - It's still lazy, but *I see why* now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### error is ALWAYS the difference between the prediction and target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> I forgot that simplest of ideas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "## Backwards pass\n",
    "## TODO: Calculate output error\n",
    "error = target - output\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### inlining a sigmoid can be justified\n",
    "but I needed a comment to do so\n",
    "\n",
    "- It's so easy to shoot yourself in the foot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# TODO: Calculate error term for output layer\n",
    "# inline f'(h) of sigmoid (output * (1 - output))\n",
    "output_error_term = error * output * (1 - output)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lines of code that are *far* too long\n",
    "I had forgotten to apply the sigmoid derivate\n",
    "\\begin{equation}\n",
    "f'(h) = f(h)(1 - f(h))\n",
    "\\end{equation}\n",
    "... against the hidden layer and had to debug bug for about 5 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in the following code block:\n",
    "\n",
    "- $f'(h)$ = `hidden_layer_output * (1 - hidden_layer_output)`\n",
    "- $o^o$ = `output_error_term`\n",
    "- $W$ = weight between current layer and previous\n",
    "  - `weights_hidden_output` __in this case__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# TODO: Calculate error term for hidden layer\n",
    "deriv_hidden_outputs = hidden_layer_output * (1 - hidden_layer_output)\n",
    "hidden_error_term = weights_hidden_output * output_error_term * deriv_hidden_outputs\n",
    "print('hidden error term:', hidden_error_term)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shaping issue occured, because I'm a know-it-all half the time.\n",
    "\n",
    "- On second edit, I added the dimensionality to our inputs $x_i$ and everything was kosher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# TODO: Calculate change in weights for hidden layer to output layer\n",
    "delta_w_h_o = learnrate * output_error_term * hidden_layer_output\n",
    "\n",
    "# TODO: Calculate change in weights for input layer to hidden layer\n",
    "delta_w_i_h = learnrate * hidden_error_term * x[:, None]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing backpropogation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what do we know?\n",
    "\n",
    "- Error in the output layer\n",
    "- Error in a given hidden layer\n",
    "\n",
    "A __remember__: we care about the error, so we can correct our predictions, increase accuracy, and automate the world :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error in the output layer\n",
    "\n",
    "The \"error term\" defined for, a given output node $k$, is\n",
    "\n",
    "\\begin{equation}\n",
    "\\delta_k = (y_k - \\hat{y}_k)f'(a_k)\n",
    "\\end{equation}\n",
    "\n",
    "- Remember that $a_k$ is the activation (output, discharge, ejaculate, etc.) of that node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error in the hidden layer\n",
    "Defined for a given hidden node $j$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\delta_j = \\sum [w_{jk}\\delta_k]f'(h_j)\n",
    "\\end{equation}\n",
    "\n",
    "- Wrote that from memory by talking to myself :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A general algorithm\n",
    "... for a simple network with one hidden layer and one output node\n",
    "\n",
    "### 1. Set the weights for each layer to zero\n",
    "- The input-to-hidden-node weights $\\Delta w_{ij} = 0$\n",
    "- The hidden-to-output-node weights $\\Delta Wj = 0$\n",
    "- __Thought__: These might be programmatically organized (like a `Map a Map ...` [Haskell syntax])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. For each record in the data, do the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Forward pass for $\\hat{y}$\n",
    "#### 2. Calculate error $y - \\hat{y}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Calculate error __gradient__\n",
    "  - This is our error (we just calculated) times the derivative of the output's activation function\n",
    "    - $\\delta^o = (y - \\hat{y})f'(z)$\n",
    "  - That derivate of action function should take a parameter $z$\n",
    "    - This $z$ is equivalent to the sum of all weighted *activations*\n",
    "    - Thus, scaling the outputs based on their corresponding inputs' weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Propogate the errors to the hidden layer\n",
    "- Where the error for any given $h$ (weighted hidden input) is multiplied by the output layer's error gradient and the weights for the given node $j$ in the hidden layer\n",
    "  - In math, it may be more clear as $\\delta_j^h = \\delta^o W_j f'(h_j)$\n",
    "    - __the error term__ for a given hidden node is equivalent to the output layer's error term $\\delta^o$ times the weights for that given node $W_j$ times the derivative of the activation function of the weighted input $h_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Update weight steps\n",
    "  - This is always $old + correction$\n",
    "    - Where $correction = errorTerm * activation$\n",
    "  - Or:\n",
    "    - $\\Delta W_j = \\Delta W_jPrevious + \\delta^o a_j$\n",
    "    - $\\Delta w_{ij} = \\Delta w_{ij} + \\delta_j^h a_i$\n",
    "      - $i$ for *i*nput  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Update weights\n",
    "  - This is always $old + scaledStep/m$\n",
    "    - Where $m$ = number of records (gotta be consistent with the math)\n",
    "    - and $\\eta$ is our multiplier/accelerator friend\n",
    "  - Or:\n",
    "    - $W_j = W_jPrevious + \\eta \\Delta W_j / m$\n",
    "    - SAME with $w$ instead of $W$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One last exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, the skeleton code:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from data_prep import features, targets, features_test, targets_test\n",
    "\n",
    "np.random.seed(21)\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "n_hidden = 2  # number of hidden units\n",
    "epochs = 900\n",
    "learnrate = 0.005\n",
    "\n",
    "n_records, n_features = features.shape\n",
    "last_loss = None\n",
    "# Initialize weights\n",
    "weights_input_hidden = np.random.normal(scale=1 / n_features ** .5,\n",
    "                                        size=(n_features, n_hidden))\n",
    "weights_hidden_output = np.random.normal(scale=1 / n_features ** .5,\n",
    "                                         size=n_hidden)\n",
    "\n",
    "for e in range(epochs):\n",
    "    del_w_input_hidden = np.zeros(weights_input_hidden.shape)\n",
    "    del_w_hidden_output = np.zeros(weights_hidden_output.shape)\n",
    "    for x, y in zip(features.values, targets):\n",
    "        ## Forward pass ##\n",
    "        # TODO: Calculate the output\n",
    "        hidden_input = None\n",
    "        hidden_output = None\n",
    "        output = None\n",
    "\n",
    "        ## Backward pass ##\n",
    "        # TODO: Calculate the network's prediction error\n",
    "        error = None\n",
    "\n",
    "        # TODO: Calculate error term for the output unit\n",
    "        output_error_term = None\n",
    "\n",
    "        ## propagate errors to hidden layer\n",
    "\n",
    "        # TODO: Calculate the hidden layer's contribution to the error\n",
    "        hidden_error = None\n",
    "        \n",
    "        # TODO: Calculate the error term for the hidden layer\n",
    "        hidden_error_term = None\n",
    "        \n",
    "        # TODO: Update the change in weights\n",
    "        del_w_hidden_output += 0\n",
    "        del_w_input_hidden += 0\n",
    "\n",
    "    # TODO: Update weights\n",
    "    weights_input_hidden += 0\n",
    "    weights_hidden_output += 0\n",
    "\n",
    "    # Printing out the mean square error on the training set\n",
    "    if e % (epochs / 10) == 0:\n",
    "        hidden_output = sigmoid(np.dot(x, weights_input_hidden))\n",
    "        out = sigmoid(np.dot(hidden_output,\n",
    "                             weights_hidden_output))\n",
    "        loss = np.mean((out - targets) ** 2)\n",
    "\n",
    "        if last_loss and last_loss < loss:\n",
    "            print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n",
    "        else:\n",
    "            print(\"Train loss: \", loss)\n",
    "        last_loss = loss\n",
    "\n",
    "# Calculate accuracy on test data\n",
    "hidden = sigmoid(np.dot(features_test, weights_input_hidden))\n",
    "out = sigmoid(np.dot(hidden, weights_hidden_output))\n",
    "predictions = out > 0.5\n",
    "accuracy = np.mean(predictions == targets_test)\n",
    "print(\"Prediction accuracy: {:.3f}\".format(accuracy))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The supporting files can be found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
